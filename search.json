[{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"version-101","dir":"","previous_headings":"Change log","what":"Version 1.0.1","title":"NA","text":"Updates (mostly) required run OSPAR 2024 CEMP assessment.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"data-import","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Data import","title":"NA","text":"OSPAR HELCOM style assessments, data Germany now matched stations name 2023 onwards. applies biota, sediment water. Note HELCOM, biota data Germany already matched name years.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"uncertainty-processing","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Uncertainty processing","title":"NA","text":"harsat 1.0.0 replaced implausibly large relative uncertainties (>=>= 100%) replaced imputed values. However, implausibly small relative uncertainties changed. code now replaces relative uncertainties <=<= 1% imputed values. defaults can changed using control$relative_uncertainty read_data. replicate defaults harsat 1.0.0, set control$relative_uncertainty = c(0, 100). keep uncertainties, regardless ridiculous , set control$relative_uncertainty = c(0, Inf). Two minor bug fixes: relative uncertainties filtered distributional types, reliable procedure determinands distribution == \"lognormal\"; checks now applied lognormal data biological effect data distributions normal lognormal incorrectly deleted; now corrected oddity files updated show: implausible_uncertainties_reported.csv - reported uncertainties replaced imputed values missing_uncertainties.csv - uncertainties (normal lognormal data) reported can’t imputed implausible_uncertaintes_calculated.csv - uncertainties calculated data processing (e.g. normalisation) implausible set missing","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"uncertainty-coefficients","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Uncertainty coefficients","title":"NA","text":"function ctsm_uncrt_workup related supporting functions used OSPAR assessments update fixed proportional standard deviations subsequently used impute missing uncertainties. functions ignored initial development harsat now harsat compatible.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"biological-effect-assessments","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Biological effect assessments","title":"NA","text":"Imposex assessments: now fully reproducible seeds random number generation provided calls ctsm.VDS.cl assess_imposex Assessment functions negative binomial data added. Negative binomial data includes MNC - number micronucleated cells.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"reporting","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Reporting","title":"NA","text":"report_assessment generates default file names. based series identifier additional station information. now possible override behaviour single report providing different file name using output_file argument.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"reference-tables","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Reference tables","title":"NA","text":"new values added method_extraction table","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"minor-bug-fixes","dir":"","previous_headings":"Change log > Version 1.0.1","what":"Minor bug fixes","title":"NA","text":"correct behaviour argument return_early create_timeseries pass info component harsat object determinand.link.sum, determinand.link.replace, determinand.link.imposex ensure early return ctsm_convert_basis nothing convert (avoids issues e.g. data biological effects) ensure SURVT (pargroup B-BIO) recognised biological effect ctsm_get_datatype (SURVT determinand pargroup isn’t auxiliary variable) pass good_status assessment functions data distributions normal lognormal trap pathological case estimation prtrend; see #436 ensure ctsm_OHAT_legends uses symbology specified write_summary_table","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"version-100","dir":"","previous_headings":"Change log","what":"Version 1.0.0","title":"NA","text":"Initial public release","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"version-013","dir":"","previous_headings":"Change log","what":"Version 0.1.3","title":"NA","text":"Various fixes","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"version-012","dir":"","previous_headings":"Change log","what":"Version 0.1.2","title":"NA","text":"Fixed issues packaged; see #326, #328 Updated AMAP data packaging; see #329","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"version-011","dir":"","previous_headings":"Change log","what":"Version 0.1.1","title":"NA","text":"Fixed issue auxiliary variables: see #289 Small documentation improvements Added build processes package bundles","code":""},{"path":"http://osparcomm.github.io/HARSAT/CHANGES.html","id":"version-010","dir":"","previous_headings":"Change log","what":"Version 0.1.0","title":"NA","text":"Initial release","code":""},{"path":[]},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"Contributing","what":"Issues","title":"Contributor’s Guide","text":"can report issues harsat using Github. Simply use Issues tab choose New issue. report issue, ’d like answer following questions. version harsat using? (’s DESCRIPTION file) version R using? operating system using? observing appears incorrect, expecting see? last point, information can give us, better. can copy paste R console logs helps tremendously, ’re 30 lines , ’s better attach file, rather pasting issue directly.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"Contributing","what":"Pull requests","title":"Contributor’s Guide","text":"welcome pull requests. easiest way fork repository Github, make changes want fork, push Github, create pull request using web interface. sent CODEOWNERS review pull request approved/rejected appropriate.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Contributor’s Guide","text":"documentation held within Github repository. use following flow. roxygen2 used source-code documentation. several vignettes vignettes directory. run harsat code (ones matching *.Rmd.orig) therefore precompiled, can take 15-20 minutes run. turns *.Rmd.orig corresponding *.Rmd files. , normal documentation building vignettes installation deploy files, along basic *.Rmd vignettes run harsat code. web site built using pkgdown – happens automatically pull requests merged main. documents code – run roxygen2 – need , manually. push main, web documentation used update GitHub Pages site: https://osparcomm.github.io/HARSAT/ – happen automatically use git flow style release process. Note pkgdown action – stored Github .github/workflows/pkgdown.yaml also creates zip files common data configuration setups. copied deployment Github Pages, can downloaded directly web pages. actions drive zipping require little care, although continue work fine simply change files data reference file directories.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"documentation-build-process","dir":"","previous_headings":"Documentation","what":"Documentation build process","title":"Contributor’s Guide","text":"documentation entirely built automatically. intentional, , example, running tools (e.g., roxygen2) can change key package files breaking way. Also, vignettes pre-rendered, take long time install users. update documentation, use following steps R. change namespacing, API changes can cause vignette builds break. , create issue. Step 1. Run roxygen2 Step 2. Precompile vignettes bash: Step 3. Build check code Back R:","code":"library(devtools) roxygen2::roxygenize() R --quiet --vanilla < vignettes/precompile.R library(devtools) devtools::check()"},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"release-process","dir":"","previous_headings":"Documentation","what":"Release process","title":"Contributor’s Guide","text":"release process less follows. adapted Stack Overflow answer. Let’s assume current version x.y.z","code":"git checkout develop git flow release start x.y.z+1 emacs DESCRIPTION     ## Update version x.y.z-9000 -> x.y.z+1 emacs CHANGES.md      ## Update the change log, as appropriate R --quiet --vanilla < vignettes/precompile.R R CMD build . R CMD check --no-manual harsat_x.y.z+1.tar.gz git add DESCRIPTION git commit -m \"Updated version to x.y.z+1 git flow release finish x.y.z+1 git checkout main git push git push --tags git push upstream git push --tags upstream git checkout develop emacs DESCRIPTION      ## Bump version to x.y.(z+1)-9000 git commit -am \"Bump develop version [ci skip]\" git push git push upstream"},{"path":"http://osparcomm.github.io/HARSAT/articles/datasets.html","id":"datasets","dir":"Articles","previous_headings":"","what":"Datasets","title":"Datasets","text":"harsat package provides three standard datasets help get started. aren’t R data files, instead zip files contain everything need analyses.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/datasets.html","id":"helcom","dir":"Articles","previous_headings":"Datasets","what":"HELCOM","title":"Datasets","text":"Download data Worked example","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/datasets.html","id":"ospar","dir":"Articles","previous_headings":"Datasets","what":"OSPAR","title":"Datasets","text":"Download data Worked example","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/datasets.html","id":"amap-external","dir":"Articles","previous_headings":"Datasets","what":"AMAP (external)","title":"Datasets","text":"Download data Worked example","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/example_HELCOM.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"HELCOM example usage","text":"vignette shows assessment (mostly) following approach taken HELCOM HOLAS3. data extracted ICES data base using XHAT facilities ICES webservice. data extracted 28 August 2023 filtered using is_helcom_area = TRUE anad maxYear = 2020. data subsequently reduced size make manageable example. data scrutinised data assessors use, results , must treated illustrative ; particular, used formal reporting. two exceptions HOLAS3 approach. First, imposex data assessed - additional level complexity explained subsequent vignette (written yet). Second, method dealing ‘initial’ data, unique HELCOM, implemented (yet available harsat). ’ll begin contaminants water simplest data assess. ’ll move sediment biota features consider. get , need set environment. First, load harsat library (let’s assume already installed , covered Getting Started guide). set main directory find data files. use data files, need point directory containing copy. usually putting data files directory data, information files directory information, can use directory .","code":"library(harsat) working.directory <- 'C:/Users/robfr/Documents/HARSAT/HARSAT'"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_HELCOM.html","id":"water-assessment","dir":"Articles","previous_headings":"","what":"Water assessment","title":"HELCOM example usage","text":"First, use read_data() read contaminant data, station dictionary, two important reference tables: determinand threshold reference tables. several things note function call: purpose = \"HELCOM\" means assessment configuration set mirror HOLAS3 assessment. can change assessment configuration many ways using control argument, explained . data_dir identifies directory storing contaminant data (water.txt) station dictionary (stations.txt). Using file.path prevents difficulties using forward backward slashes writing file paths. info_dir identifies directory storing reference tables. must named determinand.csv thresholds_water.csv. determinand table identifies determinands going assessed. thresholds, doesn’t need file called thresholds_water.csv. don’t specify extraction date, can help keep track things. well reading contaminant data station dictionary, function matches record contaminant data station station dictionary. process quite complicated (can take minutes run) don’t go details . Finally, message printed saying argument max_year set 2020. (since set maxYear 2020 data extraction.) important consequence contaminant time series assessed data period 2015 2020 (.e. last six monitoring years). next simplify data correct format running assessment. also involves deleting data meet conditions assessment. Notice message appears ‘oddities’. tidy_data create_timeseries (next function call) lot checking data strange values written oddities folder look . hope , errors, get corrected resubmitted ICES database. turns strange value stage, step follows. now data cleaning group data time series. time series consists measurements single determinand single monitoring station. Measurements filtered unfiltered samples split different time series. determinands.control argument important know since allows related determinands processed various ways. , just used PFOS linear branched components N-PFOS BR-PFOS. PFOS measurements can sumbitted PFOS (sum two components) N-PFOS BR-PFOs (individual components). determinands.control argument tells code sum records N-PFOS BR-PFOS sample relabel PFOS. complicated examples use determinands.control sediment biota examples . want see list time series, can run code along following lines: last time run assessment. need specify thresholds use, otherwise code use . water EQS, still need specify . Look threshold reference table see available unsure. Note AC stands Assessment Criteria thresholds often called. parallel argument tells code use parallel processing. usually speeds things considerably. assessment took 1.5 minutes run laptop. now need check whether convergence issues. Lack convergence rarely occurs, typically errors data (e.g. reported incorrect units) outliers, best thing first check data. However, convergence can also difficult lot less-measurements time series. Describing tweak control arguments get convergence beyond scope vignette (need write another vignette discuss ). Fortunately, convergence issues . time look results! can plot data time series along fitted model (see vignette external data) print table giving summary information assessment time series. first, may need create output directory. Now output directory, can write summary table directory. course, can choose directory write , long sure exists. code prints CSV file giving summary information assessment time series. includes: meta-data monitoring location number years data time series fitted values last monitoring year associated upper one-sided 95% confidence limits trend assessments (p-values trend estimates) status assessments (thresholds) (optionally) symbology summarising trend (shape) status (colour) time series function actively developed function arguments likely evolve, ’ll leave explanation next release.","code":"water_data <- read_data(   compartment = \"water\",    purpose = \"HELCOM\",                                  contaminants = \"water.txt\",    stations = \"stations.txt\",    data_dir = file.path(working.directory, \"data\", \"example_HELCOM\"),   info_dir = file.path(working.directory, \"information\", \"HELCOM_2023\"),    extraction = \"2023/08/23\" ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\species.csv  #> Found in path thresholds_water.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\thresholds_water.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\determinand.csv': '4b48cbec9c71380f4b464779e643cab2' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\thresholds_water.csv': '7e9487630022c11b0c3dd6d553a9955b' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/stations.txt': 'd229a1c984d507537840e73080f3773c' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/water.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/water.txt': 'b18b0556f6f78378c6f0a77682f51988' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: HELCOM  #>  - no restriction of stations by data type #>  - no restriction of stations by purpose of monitoring #>  - no restriction of data by program governance #>  - no restriction of stations by program governance #>  - matching 10846 records by coordinates #>  - matching 0 records by station name #>  - 10746 of 10846 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2020 water_data <- tidy_data(water_data) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Dropping 4071 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth >5.5m #>  - filtration missing #>  - matrix = 'SPM' #>  #> Dropping 88 records from data that have no valid station code #>  #> Dropping 13667 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data water_timeseries <- create_timeseries(   water_data,   determinands.control = list(     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\")   ) ) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2015 and 2020 #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Dropping groups of compounds / stations with no data between 2015 and 2020 get_timeseries(water_timeseries) |> head(10)  #>    station_code station_name   country determinand filtration subseries basis #> 1         11241         64A2 Lithuania          CD   filtered      <NA>     W #> 2         11241         64A2 Lithuania          PB   filtered      <NA>     W #> 3         11241         64A2 Lithuania        PFOS unfiltered      <NA>     W #> 4         11241         64A2 Lithuania       TBSN+ unfiltered      <NA>     W #> 5         12896         AUDF   Estonia          CD   filtered      <NA>     W #> 6         12896         AUDF   Estonia          PB   filtered      <NA>     W #> 7         12896         AUDF   Estonia        PFOS unfiltered      <NA>     W #> 8         12896         AUDF   Estonia       TBSN+ unfiltered      <NA>     W #> 9          2144      OMMVUW4   Germany          CD   filtered      <NA>     W #> 10         2144      OMMVUW4   Germany          PB   filtered      <NA>     W #>    unit #> 1  ug/l #> 2  ug/l #> 3  ng/l #> 4  ng/l #> 5  ug/l #> 6  ug/l #> 7  ng/l #> 8  ng/l #> 9  ug/l #> 10 ug/l water_assessment <- run_assessment(   water_timeseries,    AC = \"EQS\",    parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(water_assessment) #> All assessment models have converged summary.dir <- file.path(working.directory, \"output\", \"example_HELCOM\") if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) } write_summary_table(   water_assessment,   determinandGroups = list(     levels = c(\"Metals\", \"Organotins\", \"Organofluorines\"),      labels = c(\"Metals\", \"Organotins\", \"Organofluorines\")   ),   symbology = list(     colour = list(       below = c(\"EQS\" = \"green\"),        above = c(\"EQS\" = \"red\"),        none = \"black\"     )   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_HELCOM.html","id":"sediment-assessment","dir":"Articles","previous_headings":"","what":"Sediment assessment","title":"HELCOM example usage","text":"sediment assessment similar, extra features related normalisation (account differences grain size) described sediment data grouped time series consist measurements single determinand single matrix (fraction sample sieved ; e.g. SED63 SEDTOT) single monitoring station. create_timeseries() call sediment differs water two ways. First, determinands.control identifies two groups determinands need summed. Second, arguments normalise normalise.control specify normalisation grain size carried . default functions normalisation work many cases. However, process HELCOM complicated (unlike metals, copper normalised organic carbon rather aluminium) customised function normalise_sediment_HELCOM() provided. argument normalise.control specifies metals (apart copper) normalised 5% aluminium copper organics normalised 5% organic carbon. normalise functions need bit work, expect change. contaminant time series sediment assessed dry weight basis. measurements submitted wet weight basis converted dry weight basis using DRYWT% supporting information (also submitted data). Now run assessment. one threshold, EQS. takes minute run laptop. Everything converged. Finally, can plot individual time series assessments (see vignette external data) print summary table","code":"sediment_data <- read_data(   compartment = \"sediment\",   purpose = \"HELCOM\",   contaminants = \"sediment.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_HELCOM\"),   info_dir = file.path(working.directory, \"information\", \"HELCOM_2023\"),   extraction = \"2023/08/23\" ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\species.csv  #> Found in path thresholds_sediment.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\thresholds_sediment.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\determinand.csv': '4b48cbec9c71380f4b464779e643cab2' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv': '28e38bdd0b9e735643c60026dcda8a78' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv': '23ca1799017bfea360d586b1a70bffd4' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\thresholds_sediment.csv': '41c686160bc8e0877477239eec0f0b1b' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/stations.txt': 'd229a1c984d507537840e73080f3773c' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/sediment.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/sediment.txt': 'a5635836e9a69f3dd8be42d5078cad6b' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: HELCOM  #>  - no restriction of stations by data type #>  - no restriction of stations by purpose of monitoring #>  - no restriction of data by program governance #>  - no restriction of stations by program governance #>  - matching 6995 records by coordinates #>  - matching 9365 records by station name #>  - 15721 of 16360 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2020  sediment_data <- tidy_data(sediment_data) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Dropping 2176 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth > 0m #>  - unusual matrix #>  #> Dropping 523 records from data that have no valid station code #>  #> Dropping 13326 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data sediment_timeseries <- create_timeseries(   sediment_data,   determinands.control = list(     SBDE6 = list(       det = c(\"BDE28\", \"BDE47\", \"BDE99\", \"BD100\", \"BD153\", \"BD154\"),        action = \"sum\"     ),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\")   ),   normalise = normalise_sediment_HELCOM,   normalise.control = list(     metals = list(method = \"pivot\", normaliser = \"AL\", value = 5),      copper = list(method = \"hybrid\", normaliser = \"CORG\", value = 5),     organics = list(method = \"simple\", normaliser = \"CORG\", value = 5)    ) ) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2015 and 2020  #>    Dropping samples with only auxiliary variables #>    Relabelling matrix SED62 as SED63 and SED500, SED1000, SED2000 as SEDTOT #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Non-positive quantification limits: see non_positive_quant_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Implausible uncertainties reported with data: see implausible_uncertainties_reported.csv #>    Data submitted as BDE28, BDE47, BDE99, BD100, BD153, BD154 summed to give  #> SBDE6 #>      61 of 124 samples lost due to incomplete submissions #>    Data submitted as HBCDA, HBCDB, HBCDG summed to give HBCD #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Losing 6 out of 3282 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Normalising copper to CORG using pivot values #>    Removing sediment data where normaliser is a less than #>    Normalising metals to AL using pivot values #>    Normalising organics to 5% CORG #>    Removing sediment data where normaliser is a less than #>    Implausible uncertainties calculated in data processing: see implausible_uncertainties_calculated.csv #>    Dropping groups of compounds / stations with no data between 2015 and 2020 sediment_assessment <- run_assessment(   sediment_timeseries,    AC = \"EQS\",   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(sediment_assessment) #> All assessment models have converged write_summary_table(   sediment_assessment,   determinandGroups = webGroups <- list(     levels = c(\"Metals\", \"Organotins\", \"PAH_parent\", \"PBDEs\", \"Organobromines\"),       labels = c(       \"Metals\", \"Organotins\", \"Polycyclic aromatic hydrocarbons\",         \"Organobromines\", \"Organobromines\"      )   ),   symbology = list(     colour = list(       below = c(\"EQS\" = \"green\"),        above = c(\"EQS\" = \"red\"),        none = \"black\"     )   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_HELCOM.html","id":"biota-assessment","dir":"Articles","previous_headings":"","what":"Biota assessment","title":"HELCOM example usage","text":"main difference biota assessment inclusion effects data. example PAH metabolite time series, imposex data excluded keep things relatively simple. Imposex assessments additional modelling stage described another vignette (yet available). first two stages just construction time series features. However, first need provide individual TEQs allow construction TEQ dioxins, furans planar PCBS (labelled TEQDFP). values human health QS. stage won’t necessary later releases. biota data grouped time series consist measurements single determinand single matrix (tissue type; e.g.  ‘EH’, LI' orSB) single species single monitoring station. PAH metabolite data grouped bymethod_analysis`. determinands.control argument rather . four summed variables: PFOS, SBDE6, HBCD SCB6. also one variable CB138+163 needs relabeled (replaced ) CB138. purposes assessment, contribution CB163 regarded small. Similarly CB138+163 taken good proxy CB138. Note replacements must done six PCBs summed give SCB6 order included sum. also two ‘bespoke’ actions determinands.control. customised functions complicated things. One computes TEQ dioxins, furans planar PCBS (labelled TEQDFP) using TEFs human health QS. deals three different ways lipid weight measurements can submitted. Finally, normalise_biota_HELCOM() customised function determines measurements normalised 5% lipid HELCOM assessment. , normalisation functions active development might well change next release. One thing obvious function call choice basis. default, contaminant time series biota assessed wet weight basis, measurements submitted dry lipid weight basis transformed wet weight basis using supporting DRYWT% LIPIDWT% information. Look OSPAR external data examples see choice basis can changed. asssessment took 4.3 minutes laptop One time series converged. (parameter estimates fine, standard errors implausibly tight - look data, see routines struggle time series.) code tweaks arguments numerical differencing routine calculates standard errors. Dealing non-converged timeseries topic future vignette. ’s :)","code":"biota_data <- read_data(   compartment = \"biota\",    purpose = \"HELCOM\",                                  contaminants = \"biota.txt\",    stations = \"stations.txt\",    data_dir = file.path(working.directory, \"data\", \"example_HELCOM\"),   info_dir = file.path(working.directory, \"information\", \"HELCOM_2023\"),    extraction = \"2023/08/23\" ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\species.csv  #> Found in path thresholds_biota.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\thresholds_biota.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\determinand.csv': '4b48cbec9c71380f4b464779e643cab2' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\species.csv': '769328e51065226809c91944b6d8fe79' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv': 'b602a882d4783085c896bcf130c8f848' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\HELCOM_2023\\thresholds_biota.csv': '9af82cd9730c0b135edd4a003724e8a6' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/stations.txt': 'd229a1c984d507537840e73080f3773c' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/biota.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_HELCOM/biota.txt': '0a1a33c4e668e63c97a6d50cdc644d22' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: HELCOM  #>  - no restriction of stations by data type #>  - no restriction of stations by purpose of monitoring #>  - no restriction of data by program governance #>  - no restriction of stations by program governance #>  - matching 8644 records by coordinates #>  - matching 19893 records by station name #>  - 28437 of 28537 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2020  biota_data <- tidy_data(biota_data) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Dropping 32 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - species missing #>  #> Dropping 99 records from data that have no valid station code #>  #> Dropping 13664 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data biota_timeseries <- create_timeseries(   biota_data,   determinands.control = list(     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     SBDE6 = list(       det = c(\"BDE28\", \"BDE47\", \"BDE99\", \"BD100\", \"BD153\", \"BD154\"),        action = \"sum\"     ),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\"),     CB138 = list(det = \"CB138+163\", action = \"replace\"),     SCB6 = list(       det = c(\"CB28\", \"CB52\", \"CB101\", \"CB138\", \"CB153\", \"CB180\"),        action = \"sum\"     ),     TEQDFP = list(       det = names(info_TEF$DFP_human_health),        action = \"bespoke\",        weights = info_TEF$DFP_human_health     ),     \"LIPIDWT%\" = list(det = c(\"EXLIP%\", \"FATWT%\"), action = \"bespoke\")   ),   normalise = normalise_biota_HELCOM,   normalise.control = list(     lipid = list(method = \"simple\", value = 5),      other = list(method = \"none\")    ) ) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2015 and 2020 #>    Unexpected or missing values for 'species_group': see species_group_queries.csv #>    Unexpected or missing values for 'matrix': see matrix_queries.csv #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Bile metabolite units changed from 'ng/g' to 'ng/ml' and from 'ug/kg' to 'ug/l' #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Non-positive quantification limits: see non_positive_quant_limits.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Implausible uncertainties reported with data: see implausible_uncertainties_reported.csv #>    Data submitted as BDE28, BDE47, BDE99, BD100, BD153, BD154 summed to give  #> SBDE6 #>      257 of 497 samples lost due to incomplete submissions #>    Data submitted as HBCDA, HBCDB, HBCDG summed to give HBCD #>      5 of 42 samples lost due to incomplete submissions #>    Data submitted as CB138+163 relabelled as CB138  #>    Data submitted as CB28, CB52, CB101, CB138, CB153, CB180 summed to give SCB6 #>      22 of 925 samples lost due to incomplete submissions #>    Data submitted as  #> CB77, CB81, CB105, CB118, CB126, CB156, CB157, CB167, CB169, CDD1N, CDD4X, CDD6P, CDD6X, CDD9X, CDDO, CDF2N, CDF2T, CDF4X, CDF6P, CDF6X, CDF9P, CDF9X, CDFO, CDFP2, CDFX1, TCDD  #> summed to give TEQDFP #>      907 of 945 samples lost due to incomplete submissions #>    Data submitted as EXLIP% or FATWT% relabelled as LIPIDWT%  #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Losing 63 out of 5737 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Normalising lipid to 5% #>    No normalisation for other #>    Dropping groups of compounds / stations with no data between 2015 and 2020 biota_assessment <- run_assessment(   biota_timeseries,    AC = c(\"BAC\", \"EAC\", \"EQS\", \"MPC\"),   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(biota_assessment) #> The following assessment models have not converged: #> 2299 PYR1OH Limanda limanda BI HPLC-FD  biota_assessment <- update_assessment(   biota_assessment,    subset = series == \"2299 PYR1OH Limanda limanda BI HPLC-FD\",   hess.d = 0.0001, hess.r = 8 ) #>  #> assessing series:  station_code 2299; determinand PYR1OH; species Limanda limanda; matrix BI; method_analysis HPLC-FD; unit ng/ml  check_assessment(biota_assessment) #> All assessment models have converged write_summary_table(   biota_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"PAH_parent\", \"Metabolites\", \"PBDEs\", \"Organobromines\",        \"Organofluorines\", \"Chlorobiphenyls\", \"Dioxins\"     ),       labels = c(       \"Metals\", \"PAH compounds and metabolites\", \"PAH compounds and metabolites\",       \"Organobromines\", \"Organobromines\", \"Organofluorines\",        \"PCBs and dioxins\", \"PCBs and dioxins\"     )   ),   symbology = list(     colour = list(       below = c(\"EQS\" = \"green\"),        above = c(\"EQS\" = \"red\"),        none = \"black\"     )   ),   collapse_AC = list(EAC = c(\"EAC\", \"EQS\", \"MPC\")),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"OSPAR example usage","text":"vignette shows assessment following approach used OSPAR 2022 assessment 2023 Qaulity Status Report. example assumes familiarity HELCOM vignette mostly describes features differ HELCOM approach. data extracted ICES data base using XHAT facilities ICES webservice. data extracted 28 August 2023 filtered using is_ospar_area = TRUE. data subsequently reduced size make manageable example. data scrutinised data assessors use, results , must treated illustrative ; particular, used formal reporting. First, need set environment loading harsat library (assuming already installed , covered Getting Started guide). set main directory find data files. use data files, need point directory containing copy. usually putting data files directory data, information files directory information, can use directory .","code":"library(harsat) working.directory <- 'C:/Users/robfr/Documents/HARSAT/HARSAT'"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"water-assessment","dir":"Articles","previous_headings":"","what":"Water assessment","title":"OSPAR example usage","text":"OSPAR water assessment similar HOLAS assessment. main difference determinands assessed means determinands.control needs work create_timeseries(). Now let’s look water_summary.csv file:","code":"water_data <- read_data(   compartment = \"water\",   purpose = \"OSPAR\",   contaminants = \"water.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),   extraction = \"2023/08/23\" ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\species.csv  #> Found in path thresholds_water.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_water.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv': '6b36346446c0ac04a52b3f1347829f6b' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_water.csv': '615ef96f716ef1d43c01ab67f383c881' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt': '58b9e90f314e89f637c60558c06755f4' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/water.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/water.txt': '13d63b6161b671165b215b58f5e22469' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CW or EW  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 2768 records by coordinates #>  - matching 1583 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 4251 of 4351 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2022  water_data <- tidy_data(water_data) #>  #> Oddities will be written to 'oddities/water' #>  #> Dropping 411 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth >5.5m #>  - filtration missing #>  - matrix = 'SPM' #>  #> Dropping 94 records from data that have no valid station code #>  #> Dropping 13633 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data water_timeseries <- create_timeseries(   water_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\"),      HCH = list(det = c(\"HCHA\", \"HCHB\", \"HCHG\"), action = \"sum\")   ) ) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2017 and 2022 #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Implausible uncertainties reported with data: see implausible_uncertainties_reported.csv #>    Data submitted as CHRTR relabelled as CHR  #>    Data submitted as BBF, BKF summed to give BBKF #>      1 of 71 samples lost due to incomplete submissions #>    Data submitted as BBJF, BKF summed to give BBJKF #>      99 of 99 samples lost due to incomplete submissions #>    Data submitted as HCEPC, HCEPT summed to give HCEPX #>    Data submitted as HCHA, HCHB, HCHG summed to give HCH #>      190 of 234 samples lost due to incomplete submissions #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Dropping groups of compounds / stations with no data between 2017 and 2022 water_assessment <- run_assessment(   water_timeseries,    AC = \"EQS\",    parallel = TRUE ) #> Setting up clusters: be patient, it can take a while!  check_assessment(water_assessment) #> All assessment models have converged summary.dir <- file.path(working.directory, \"output\", \"example_OSPAR\") if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) } write_summary_table(   water_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\", \"PAH_parent\",  \"Organofluorines\",        \"Chlorobiphenyls\", \"Organochlorines\", \"Pesticides\"     ),       labels = c(       \"Metals\", \"Organotins\", \"PAH parent compounds\", \"Organofluorines\",        \"Polychlorinated biphenyls\", \"Organochlorines (other)\", \"Pesticides\"     )   ),   symbology = list(     colour = list(       below = c(\"EQS\" = \"green\"),        above = c(\"EQS\" = \"red\"),        none = \"black\"     )   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"sediment-assessment","dir":"Articles","previous_headings":"","what":"Sediment assessment","title":"OSPAR example usage","text":"main differences sediment assessment relate normalisation procedure create_timeseries() use multiple thresholds run_assessment(). customised function normalise_sediment_OSPAR() written implement OSPAR normalisation procedure. necessary pivot values used normalising metals region-specific. aside, procedure straightforward: metals normalised 5% aluminium organics normalised 2.5% organic carbon, unless samples taken Iberian Coast Gulf Cadiz OSPAR subregions, case concentrations normalised. TEFs TEQ dioxins, furans planar PCBs (labelled TEQDFP) values secondary poisoning QS. Five thresholds applied. Background Assessment Concentrations (BACs) differ ospar_subregions. see implemented, look thresholds_sediment.csv file. Now let’s look sediment_summary.csv file:","code":"sediment_data <- read_data(   compartment = \"sediment\",    purpose = \"OSPAR\",   contaminants = \"sediment.txt\",    stations = \"stations.txt\",    data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),    extraction = \"2023/08/23\" )   #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\species.csv  #> Found in path thresholds_sediment.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_sediment.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv': '6b36346446c0ac04a52b3f1347829f6b' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv': '28e38bdd0b9e735643c60026dcda8a78' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv': '23ca1799017bfea360d586b1a70bffd4' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_sediment.csv': 'ab2fddb32a8b1d126004febbf6375b5d' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt': '58b9e90f314e89f637c60558c06755f4' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/sediment.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/sediment.txt': '0cb722db8f8ea4ed263e1cb8c8334665' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CS or ES  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 6717 records by coordinates #>  - matching 11760 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 18054 of 18477 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2021 sediment_data <- tidy_data(sediment_data) #>  #> Oddities will be written to 'oddities/sediment' #>  #> Dropping 75 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth > 0m #>  - unusual matrix #>  #> Dropping 423 records from data that have no valid station code #>  #> Dropping 13555 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data sediment_timeseries <- create_timeseries(   sediment_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     NAPC1 = list(det = c(\"NAP1M\", \"NAP2M\"), action = \"sum\"),     BD154 = list(det = \"PBB153+BD154\", action = \"replace\"),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     CB156 = list(det = c(\"CB156+172\"), action = \"replace\"),     TEQDFP = list(       det = names(info_TEF$DFP_environmental),        action = \"bespoke\",        weights = info_TEF$DFP_environmental     ),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\")   ),   normalise = normalise_sediment_OSPAR,   normalise.control = list(     metals = list(method = \"pivot\", normaliser = \"AL\", value = 5.0),      organics = list(method = \"simple\", normaliser = \"CORG\", value = 2.5),     exclude = expression(ospar_subregion %in% c(\"Iberian Coast\", \"Gulf of Cadiz\"))   ) ) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2016 and 2021  #>    Dropping samples with only auxiliary variables #>    Relabelling matrix SED62 as SED63 and SED500, SED1000, SED2000 as SEDTOT #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Missing digestion methods: see digestion_errors.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Implausible uncertainties reported with data: see implausible_uncertainties_reported.csv #>    Data submitted as BBF, BKF summed to give BBKF #>    Data submitted as BBJF, BKF summed to give BBJKF #>      104 of 104 samples lost due to incomplete submissions #>    Data submitted as NAP1M, NAP2M summed to give NAPC1 #>    Data submitted as  #> CB77, CB81, CB105, CB118, CB126, CB156, CB157, CB167, CB169, CDD1N, CDD4X, CDD6P, CDD6X, CDD9X, CDDO, CDF2N, CDF2T, CDF4X, CDF6P, CDF6X, CDF9P, CDF9X, CDFO, CDFP2, CDFX1, TCDD  #> summed to give TEQDFP #>      17 of 17 samples lost due to incomplete submissions #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Normalising metals to AL using pivot values #>    Inferring missing normaliser digestion from corresponding contaminant digestion #>    Warning: Nss values for CR hard-wired to 5.0 (AL) or 52 (LI) for all digestions #>    Normalising organics to 2.5% CORG #>    Removing sediment data where normaliser is a less than #>    Implausible uncertainties calculated in data processing: see implausible_uncertainties_calculated.csv #>    Dropping groups of compounds / stations with no data between 2016 and 2021 sediment_assessment <- run_assessment(   sediment_timeseries,    AC = c(\"BAC\", \"EAC\", \"EQS\", \"ERL\", \"FEQG\"),   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while!  check_assessment(water_assessment) #> All assessment models have converged write_summary_table(   sediment_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\", \"PAH_parent\", \"PAH_alkylated\",         \"PBDEs\", \"Organobromines\", \"Chlorobiphenyls\", \"Dioxins\",        \"Organochlorines\"     ),     labels = c(       \"Metals\", \"Organotins\", \"PAH parent compounds\", \"PAH alkylated compounds\",        \"Polybrominated diphenyl ethers\", \"Organobromines (other)\",        \"Polychlorinated biphenyls\", \"Dioxins\", \"Organochlorines (other)\"     )   ),   symbology = list(     colour = list(       below = c(         \"BAC\" = \"blue\",          \"ERL\" = \"green\",          \"EAC\" = \"green\",          \"EQS\" = \"green\",          \"FEQG\" = \"green\"       ),       above = c(         \"BAC\" = \"orange\",          \"ERL\" = \"red\",          \"EAC\" = \"red\",          \"EQS\" = \"red\",          \"FEQG\" = \"red\"       ),       none = \"black\"     )   ),   collapse_AC = list(BAC = \"BAC\", EAC = c(\"EAC\", \"ERL\", \"EQS\", \"FEQG\")),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"biota-assessment","dir":"Articles","previous_headings":"","what":"Biota assessment","title":"OSPAR example usage","text":"OSPAR biota assessment procedure differs two ways HELCOM procedure. First, many determinands assessed, including greater set biological effects (although imposex measurements exluded example topic later vignette). Second, basis assessment far complicated. example also shows can break modelling stage several parts. sometimes necessary lot timeseries hit memory limitations R. Note threshold file currently incomplete. actively worked . OSPAR typically assesses contaminant concentrations shellfish dry weight basis, metal concentrations fish wet weight basis, organic concentrations fish either wet weight lipid weight basis, depending typical lipid content fish tissue. also rules birds mammals. dealt using customised function get_basis_biota_OSPAR(). , demonstrate split modelling stage assessment several parts. call run_assessment() assesses metal time series (specified metals wk_metals). call update_assessment() assesses remaining time series. general, can subset run_assessment() update_assessment() using column time series component biota_timeseries; includes determinand, matrix, species , special case, series, just want update one two time series. latter particularly useful series converged want rerun series different optimisation controls (subject separate vignette). application thresholds also complicated can handled standard routines. example, Background Assessment Concentrations cadmium lead fish applied species / tissue combinations lipid content >=>= 3%. dealt using customised function get_AC_biota_OSPAR(). One time series converged. (parameter estimates fine, standard errors available - look data, see routines struggle time series.) code tweaks arguments numerical differencing routine calculates standard errors. Dealing non-converged timeseries topic future vignette. now write summary table. symbology based environmental thresholds. separate call needed get symbology based health thresholds. symbology functionality still development intended allow sets symbologies calculated single call. finally, let’s take look biota_summary.csv file.","code":"biota_data <- read_data(   compartment = \"biota\",   purpose = \"OSPAR\",   contaminants = \"biota.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),    extraction = \"2023/08/23\" ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\species.csv  #> Found in path thresholds_biota.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_biota.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv': '6b36346446c0ac04a52b3f1347829f6b' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\species.csv': '952a6e718e07b8bc501eafe42a74a760' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv': 'b602a882d4783085c896bcf130c8f848' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_biota.csv': 'a487aa3bb6738f95ab9462e4420e124a' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt': '58b9e90f314e89f637c60558c06755f4' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/biota.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/biota.txt': 'abfd256bda44cde5ed63693c75ec9024' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CF or EF  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 10047 records by coordinates #>  - matching 66378 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 75965 of 76425 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2022 biota_data <- tidy_data(biota_data) #>  #> Oddities will be written to 'oddities/biota' #>  #> Dropping 11 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - species missing #>  #> Dropping 460 records from data that have no valid station code #>  #> Dropping 13427 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data biota_timeseries <- create_timeseries(   biota_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     NAPC1 = list(det = c(\"NAP1M\", \"NAP2M\"), action = \"sum\"),     BD154 = list(det = \"PBB153+BD154\", action = \"replace\"),     SBDE6 = list(       det = c(\"BDE28\", \"BDE47\", \"BDE99\", \"BD100\", \"BD153\", \"BD154\"),        action = \"sum\"     ),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\"),     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     PFHXS = list(det = c(\"N-PFHXS\", \"BR-PFHXS\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     CB156 = list(det = c(\"CB156+172\"), action = \"replace\"),     SCB6 = list(       det = c(\"CB28\", \"CB52\", \"CB101\", \"CB138\", \"CB153\", \"CB180\"),        action = \"sum\"     ),     SCB7 = list(       det = c(\"CB28\", \"CB52\", \"CB101\", \"CB118\", \"CB138\", \"CB153\", \"CB180\"),        action = \"sum\"     ),     TEQDFP = list(       det = names(info_TEF$DFP_environmental),        action = \"bespoke\",        weights = info_TEF$DFP_environmental     ),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\"),     HCH = list(det = c(\"HCHA\", \"HCHB\", \"HCHG\"), action = \"sum\"),     \"LIPIDWT%\" = list(det = c(\"EXLIP%\", \"FATWT%\"), action = \"bespoke\")   ),    get_basis = get_basis_biota_OSPAR ) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2017 and 2022  #>    Dropping samples with only auxiliary variables #>    Unexpected or missing values for 'sex': see sex_queries.csv #>    Unexpected or missing values for 'matrix': see matrix_queries.csv #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Bile metabolite units changed from 'ng/g' to 'ng/ml' and from 'ug/kg' to 'ug/l' #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Unrecognised censoring values: deleted data in 'censoring_codes_unrecognised.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Implausible uncertainties reported with data: see implausible_uncertainties_reported.csv #>    Data submitted as CHRTR relabelled as CHR  #>    Data submitted as BBF, BKF summed to give BBKF #>      125 of 149 samples lost due to incomplete submissions #>    Data submitted as BBJF, BKF summed to give BBJKF #>      27 of 127 samples lost due to incomplete submissions #>    Data submitted as BBJKF relabelled as BBKF  #>    Data submitted as NAP1M, NAP2M summed to give NAPC1 #>      1 of 1 samples lost due to incomplete submissions #>    Data submitted as BDE28, BDE47, BDE99, BD100, BD153, BD154 summed to give  #> SBDE6 #>      92 of 204 samples lost due to incomplete submissions #>    Data submitted as HBCDA, HBCDB, HBCDG summed to give HBCD #>      10 of 51 samples lost due to incomplete submissions #>    Data submitted as CB28, CB52, CB101, CB138, CB153, CB180 summed to give SCB6 #>      26 of 97 samples lost due to incomplete submissions #>    Data submitted as CB28, CB52, CB101, CB118, CB138, CB153, CB180  #> summed to give SCB7 #>      26 of 97 samples lost due to incomplete submissions #>    Data submitted as  #> CB77, CB81, CB105, CB118, CB126, CB156, CB157, CB167, CB169, CDD1N, CDD4X, CDD6P, CDD6X, CDD9X, CDDO, CDF2N, CDF2T, CDF4X, CDF6P, CDF6X, CDF9P, CDF9X, CDFO, CDFP2, CDFX1, TCDD  #> summed to give TEQDFP #>      114 of 114 samples lost due to incomplete submissions #>    Data submitted as HCHA, HCHB, HCHG summed to give HCH #>      349 of 840 samples lost due to incomplete submissions #>    Data submitted as EXLIP% or FATWT% relabelled as LIPIDWT%  #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Losing 1197 out of 11234 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Dropping bivalve and gastropod contaminant data collected during the #>     spawning season, which is taken to be the following months: #>     April, May, June, July  #>    Dropping groups of compounds / stations with no data between 2017 and 2022 wk_metals <-    c(\"AG\", \"AS\", \"CD\", \"CO\", \"CR\", \"CU\", \"HG\", \"NI\", \"PB\", \"SE\", \"SN\", \"ZN\")  biota_assessment <- run_assessment(   biota_timeseries,    subset = determinand %in% wk_metals,   AC = c(\"BAC\", \"NRC\", \"EAC\", \"FEQG\", \"LRC\", \"QSsp\", \"MPC\", \"QShh\"),   get_AC_fn = get_AC_biota_OSPAR,   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while!  biota_assessment <- update_assessment(   biota_assessment,    subset = !determinand %in% wk_metals,   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(biota_assessment) #> The following assessment models have not converged: #> 5031 BBKF Mytilus edulis SB  biota_assessment <- update_assessment(   biota_assessment,    subset = series == \"5031 BBKF Mytilus edulis SB\",   hess.d = 0.0001, hess.r = 8 ) #>  #> assessing series:  station_code 5031; determinand BBKF; species Mytilus edulis; matrix SB; basis D; unit ug/kg  check_assessment(biota_assessment) #> All assessment models have converged write_summary_table(   biota_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\",       \"PAH_parent\", \"PAH_alkylated\", \"Metabolites\",       \"PBDEs\", \"Organobromines\",       \"Organofluorines\",       \"Chlorobiphenyls\", \"Dioxins\", \"Organochlorines\",       \"Effects\"     ),     labels = c(       \"Metals\", \"Organotins\",       \"PAH parent compounds\", \"PAH alkylated compounds\", \"PAH metabolites\",       \"Polybrominated diphenyl ethers\", \"Organobromines (other)\",       \"Organofluorines\",       \"Polychlorinated biphenyls\", \"Dioxins\", \"Organochlorines (other)\",       \"Biological effects (other)\"     )   ),   symbology = list(     colour = list(       below = c(         \"BAC\" = \"blue\",         \"NRC\" = \"blue\",         \"EAC\" = \"green\",          \"FEQG\" = \"green\",         \"LRC\" = \"green\",          \"QSsp\" = \"green\"       ),       above = c(         \"BAC\" = \"orange\",          \"NRC\" = \"orange\",          \"EAC\" = \"red\",          \"FEQG\" = \"red\",         \"LRC\" = \"red\",          \"QSsp\" = \"red\"       ),       none = \"black\"     )   ),   collapse_AC = list(     BAC = c(\"BAC\", \"NRC\"),     EAC = c(\"EAC\", \"LRC\", \"QSsp\"),      HQS = c(\"MPC\", \"QShh\")   ),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"read-data","dir":"Articles","previous_headings":"","what":"Read data","title":"External data","text":"Mercury data supporting variables station dictionary Note: function, replace ‘control = list()’ ‘control = list(use_stage = TRUE)’ include subseries ICES format data runs; ‘external’ format runs, input data file includes column named ‘subseries’, data column used define subseries, exclude subseries runs column can omitted renamed e.g. ‘nosubseries’","code":"biota_data <- read_data(   compartment = \"biota\",   purpose = \"AMAP\",   contaminants = \"EXTERNAL_FO_PW_DATA.csv\",   # NB, replace biota data filename above, as appropriate    stations = \"EXTERNAL_AMAP_STATIONS.csv\",    # NB, replace station data filename above, as appropriate    data_dir = file.path(working.directory, \"data\", \"example_external_data\"),   data_format = \"external\",   info_dir = file.path(working.directory, \"information\", \"AMAP\"), ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\AMAP\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\AMAP\\species.csv  #> Found in path thresholds_biota.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\AMAP\\thresholds_biota.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\AMAP\\determinand.csv': '80bca84d428856c93e89c52aebf8b144' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\AMAP\\species.csv': '1aba5ace8155923ed18bd9d0b414e48e' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv': 'b602a882d4783085c896bcf130c8f848' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\AMAP\\thresholds_biota.csv': 'a6d82623b8968910b59c1308a646e8a8' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_external_data/EXTERNAL_AMAP_STATIONS.csv' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_external_data/EXTERNAL_AMAP_STATIONS.csv': '91e7eb7661ce43b02c68cc81153ac3d7' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_external_data/EXTERNAL_FO_PW_DATA.csv' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_external_data/EXTERNAL_FO_PW_DATA.csv': '9ade644c5bdb561f0c3bf4a3560a2c15' #>  #> Argument max_year taken to be the maximum year in the data: 2020"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"prepare-data-for-next-stage","dir":"Articles","previous_headings":"","what":"Prepare data for next stage","title":"External data","text":"Get correct variables streamline data files","code":"biota_data <- tidy_data(biota_data) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Dropping 54 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"construct-timeseries","dir":"Articles","previous_headings":"","what":"Construct timeseries","title":"External data","text":"timeseries, use basis reported often data","code":"biota_timeseries <- create_timeseries(   biota_data,   determinands = ctsm_get_determinands(biota_data$info),   determinands.control = NULL,   oddity_path = \"oddities\",   return_early = FALSE,   print_code_warnings = FALSE,   get_basis = get_basis_most_common,   normalise = FALSE,   normalise.control = list() ) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2015 and 2020 #>    Replicate measurements, only first retained: see replicate_measurements.csv #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Missing uncertainties which cannot be imputed: deleted data in 'missing_uncertainties.csv #>    Dropping groups of compounds / stations with no data between 2015 and 2020"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"assessment","dir":"Articles","previous_headings":"","what":"Assessment","title":"External data","text":"Main runs Note: control argument specifies post-hoc power metrics based power 80% annual percentage increase concentration 10%. Use code takes long time run","code":"biota_assessment <- run_assessment(   biota_timeseries,   subset = NULL,   AC = NULL,   get_AC_fn = NULL,   recent_trend = 20L,   parallel = FALSE,    extra_data = NULL,   control = list(power = list(target_power = 80, target_trend = 10))  ) #>  #> assessing series:  station_code A902; determinand BD100; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand BD153; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand BD154; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand BD183; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand BDE28; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand BDE47; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand BDE66; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand BDE85; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand BDE99; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB101; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB105; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB118; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand CB128; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB138; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB153; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB156; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand CB170; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB180; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand CB183; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand CB187; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB28; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CB52; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CCDAN; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand CD; species Globicephala melas; matrix KI; subseries AD; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CD; species Globicephala melas; matrix LI; subseries AD; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand CD; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand CNONC; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand DDEOP; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand DDEPP; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand DDTOP; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand DDTPP; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HBCDA; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HBCDB; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HBCDG; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HCHB; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HG; species Globicephala melas; matrix LI; subseries AD; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand HG; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HG; species Globicephala melas; matrix MU; subseries AD; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand HG; species Globicephala melas; matrix MU; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand OCDAN; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand PFDA; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFDA; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand PFDOA; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFDOA; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand PFDS; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFHXS; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFHXS; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand PFNA; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFNA; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFOA; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFOA; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFOS; species Globicephala melas; matrix BB; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand PFOS; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand SE; species Globicephala melas; matrix LI; subseries AD; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand SE; species Globicephala melas; matrix LI; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand SE; species Globicephala melas; matrix MU; subseries AD; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand SE; species Globicephala melas; matrix MU; subseries JV; basis W; unit ug/kg  #>  #> assessing series:  station_code A902; determinand TCDAN; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand TDEOP; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand TDEPP; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A902; determinand TNONC; species Globicephala melas; matrix BB; subseries JV; basis L; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') biota_assessment <- run_assessment(   biota_timeseries,   subset = NULL,   AC = NULL,   get_AC_fn = NULL,   recent_trend = 20L,   parallel = TRUE,   extra_data = NULL,   control = list(power = list(target_power = 80, target_trend = 10))  )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"check-convergence","dir":"Articles","previous_headings":"Assessment","what":"Check convergence","title":"External data","text":"","code":"check_assessment(biota_assessment, save_result = FALSE) #> All assessment models have converged"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"summary-files","dir":"Articles","previous_headings":"","what":"Summary files","title":"External data","text":"writes summary data file output/example_external_data. argument extra_output = \"power\" ensures power metrics lognormally distributed data exported.","code":"summary.dir <- file.path(working.directory, \"output\", \"example_external_data\")  if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) }   write_summary_table(   biota_assessment,   output_file = \"biota-FO-PW-test-output.csv\",   # NB, file will be overwritten so change name as appropriate to retain results   output_dir = summary.dir,   export = TRUE,   determinandGroups = NULL,   symbology = NULL,   collapse_AC = NULL,    extra_output = \"power\" )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"graphics-output","dir":"Articles","previous_headings":"","what":"Graphics output","title":"External data","text":"Plots assessment either data (file_type = “data”) annual index (file_type = “index”) (default) plot function writes output output_dir directory (must exist); settings plots output .png formatted graphics; function can omitted avoid graphical output; changing “png” “pdf” statement output graphics PDF (vector) format. Can subset assessment based variables either timeSeries stations components object: commonly determinand, matrix, species, station_code station_name; can also use series identifier row.names(timeSeries) subset NULL (default), timeseries plotted (can take time) Graphics plots written files output/graphics. code write summary file run concerned. Checksums four runs described example follows:","code":"graphics.dir <- file.path(working.directory, \"output\", \"graphics\")  if (!dir.exists(graphics.dir)) {   dir.create(graphics.dir, recursive = TRUE) }   plot_assessment(   biota_assessment,   subset = NULL ,   output_dir = summary.dir,   file_type = c(\"data\", \"index\"),   file_format = c(\"png\" ) )"},{"path":"http://osparcomm.github.io/HARSAT/articles/external-file-format.html","id":"contaminant-data","dir":"Articles","previous_headings":"","what":"Contaminant data","title":"External data file formats","text":"data file one row measurement.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/external-file-format.html","id":"station-data","dir":"Articles","previous_headings":"","what":"Station data","title":"External data file formats","text":"station file one row station.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"File management","text":"R package harsat designed make easy work files. harsat uses following different kinds file: Data files. stored data directory anywhere file system. actual data (contaminant measurments station dictionary) files. TAB files downloaded ICES webservice (.e. ICES format) CSV files put together (simpler external format). Analysis files. drive actual analysis. Three files particulary important: determinand file, species file (biota assessment) threshold file. normally keep analysis directory somewhere file system. CSV files, smaller won’t change anywhere near often data files. Configuration files. several additional configuration files provide information , example, chemical methods pivot values sediment normalisation. harsat package provides default versions files fine assessments. However, may override defaults putting modified copies files analysis directory described . configuration files also CSV files. Datasets page provides zip files : data assessment files vignette analysis files recent OSPAR, HELCOM AMAP assessments default additional configuration files need assemble reliable set files assessment, data directory analysis directory. support full reproducibility, good practice also put copy configuration files (whether modified ) analysis directory. updates R harsat package may change contents default configuration files. data files analysis files, copied modified configuration files put analysis directory, affected.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"file-encodings","dir":"Articles","previous_headings":"","what":"File encodings","title":"File management","text":"harsat currently expects files encoded using UTF-8. Standard ASCII files also fine, UTF-8 superset ASCII. using accented characters another encoding, common Latin-1 (also known iso-8859-1) ask convert UTF-8 first. using Microsoft Excel prepare , simply means saving CSV files UTF-8 encoding. Note: reason allow people choose file encoding option input files, harsat reads quite files, hard specify encodings different file encodings. UTF-8 standard now, automatically used ICES data anyway, harsat instead check UTF-8, files aren’t encoded expected, warn . using tools, please check documentation tools make sure aren’t converting files Latin-1 behind back. check encoding file, use file command, slightly different depending operating system. Mac: Windows Linux (Windows, may need File Windows): charset reported utf-8 (us-ascii), need convert . easiest way using iconv, built Macs, available download Windows. can use converted version file-utf8.csv workflows.","code":"$ file -I file.csv file.csv: text/csv; charset=utf-8 $ file file.csv file.csv: text/csv; charset=utf-8 iconv -f <old charset> -t utf-8 file.csv > file-utf8.csv"},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"typical-workflow","dir":"Articles","previous_headings":"","what":"Typical workflow","title":"File management","text":"Let’s imagine want run analysis harsat, already installed R package (described Getting started page). Now need data. Typically get ICES webservice, put together data files using simpler external format. now let’s imagine want try OSPAR vignette. can navigate Datasets page, look approprate zip file download OSPAR vignette. download unzip file (can unzip anywhere like, let’s pretend ’re using Windows unzip : C:\\Users\\stuart\\OSPAR_vignette) ’ll see disk contains files follows: means directories follows: Data directory: C:\\Users\\stuart\\OSPAR_vignette\\data Analysis directory: C:\\Users\\stuart\\OSPAR_vignette\\analysis Obviously, can put directories anywhere like file system. can even put removable disk like, network shared drive. can also call directories something else. example, might call data_vignette analysis_vignette distinguish assessments. , now let’s see might use run analysis.","code":"+ C:\\Users\\stuart\\OSPAR_vignette   |   + data   | |   | + test_data.csv   | + station_dictionary.csv   | + quality_assurance.csv   |   + analysis      |     + determinand.csv     + species.csv     + thresholds_biota.csv"},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"reading-your-data-files","dir":"Articles","previous_headings":"","what":"Reading your data files","title":"File management","text":"Virtually work need involves call harsat’s read_data() function. Let’s suppose R working directory (R Project) C:\\Users\\stuart\\OSPAR_vignette\\. call typically look like : default, function looks data analysis files directories called data assessment nested inside working directory. called something else, can use data_dir analysis_dir arguments. example: can also specify absolute path names important things see : Note use file.path() make portable pathnames. course, user can use whatever filename pattern works best . info_path parameter can vector well single string. harsat actually search every directory vector, looking files like determinand.csv. file found local analysis directory, gets read used. , may try directories vector. get end ’ve still found particular file (especially common standard ones like matrix.csv translates common codes) harsat’s built-directory configuration files gets used last resort. file really essential still can’t find , harsat immediately throw error can intervene. harsat , log file actually used, also log “thumbprint” file contents – typically something like string hexadecimal digits. wherever file comes , long contents file . move file different directory don’t edit , thumbprint show. , thumbprints vital tool tracking reproducibility, change contents data changes.","code":"biota_data <- read_data(   compartment = \"biota\",    purpose = \"OSPAR\",                                  contaminants = \"test_data.csv\",    stations = \"station_dictionary.csv\",    data_format = \"ICES\", ) biota_data <- read_data(   compartment = \"biota\",    purpose = \"OSPAR\",                                  contaminants = \"test_data.csv\",    stations = \"station_dictionary.csv\",   data_dir = \"data_vignette\",   data_format = \"ICES\",   analysis_dir = \"analysis_vignette\" ) biota_data <- read_data(   compartment = \"biota\",    purpose = \"OSPAR\",                                  contaminants = \"test_data.csv\",    stations = \"station_dictionary.csv\",   data_dir = file.path(\"C:\", \"Users\", \"stuart\", \"OSPAR_vignette\", \"data\"),   data_format = \"ICES\",   analysis_dir = file.path(\"C:\", \"Users\", \"stuart\", \"somewhere_else\", \"assessment\"), )"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"installing-harsat-from-a-bundled-package","dir":"Articles","previous_headings":"","what":"Installing harsat from a bundled package","title":"Getting started","text":"downloaded copy bundled package, whch file named something like harsat_0.1.2.tar.gz harsat_0.1.2.tar, can install directly, R still take care making sure right dependencies.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"in-rstudio","dir":"Articles","previous_headings":"Installing harsat from a bundled package","what":"In RStudio","title":"Getting started","text":"Packages tab, choose Install, make sure selected install Package Archive File, press Browse... button locate bundled package file. finally press Install button.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"from-the-r-command-line","dir":"Articles","previous_headings":"Installing harsat from a bundled package","what":"From the R command line","title":"Getting started","text":"Use command: Use right file downloaded package file, press enter, away go.","code":"install.packages(remotes) -- if needed library(remotes) remotes::install_local(\"~/Downloads/harsat_0.1.2.tar\")"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"installing-harsat-directly-from-github","dir":"Articles","previous_headings":"","what":"Installing harsat directly from GitHub","title":"Getting started","text":"install latest version, use remotes package: development version similar: Note: many functions currently cstm prefix. code originally developed, thought “contaminant time series modelling”, get cstm prefixes. removed near future.","code":"# install.packages(remotes) -- if needed library(remotes) remotes::install_github(\"osparcomm/harsat@main\") # install.packages(\"devtools\") devtools::install_github(\"osparcomm/HARSAT@develop\")"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"loading-the-code","dir":"Articles","previous_headings":"","what":"Loading the code","title":"Getting started","text":"Now, within R, can load library usual way","code":"library(harsat)"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"accessing-files","dir":"Articles","previous_headings":"","what":"Accessing files","title":"Getting started","text":"organize files . set main directory find data files. use data files, need point directory containing copy. usually putting data files directory data, information files directory information, can use directory .","code":"working.directory <- 'C:/Users/robfr/Documents/HARSAT/HARSAT'"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"reading-in-the-data","dir":"Articles","previous_headings":"","what":"Reading in the data","title":"Getting started","text":"first step read data ’ve got, using read_data(). go arguments. main arguments follows: compartment argument specifies whether ’re dealing biota assessment, sediment assessment, water assessment. purpose means can mirror OSPAR style assessment, HELCOM style assessment, AMAP-style assessment, means can basically tailor idea code sufficiently flexible can lot tailoring suit needs. contaminants data file chemical measurements . stations station file directly related station dictionary get ICES. info_dir directory reference tables – come back . reads three data sets, stage. get point, can look data want , anything else need data proceeding. Essentially files come point unchanged files reading . basically just reading data setting things . point might want whole lot ad hoc corrections data, done OSPAR estimates.","code":"water_data <- read_data(   compartment = \"water\",   purpose = \"OSPAR\",   contaminants = \"water.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),   extraction = \"2023/08/23\" ) #> Found in path determinand.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv  #> Found in path species.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\species.csv  #> Found in path thresholds_water.csv C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_water.csv  #> Found in package method_extraction.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/method_extraction.csv  #> Found in package pivot_values.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/pivot_values.csv  #> Found in package matrix.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv  #> Found in package imposex.csv C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/imposex.csv  #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\determinand.csv': '6b36346446c0ac04a52b3f1347829f6b' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/inst/information/matrix.csv': '4b4fb3814bb84cfbf9b37f7b59d45eb9' #> MD5 digest for: 'C:\\Users\\robfr\\Documents\\HARSAT\\HARSAT\\information\\OSPAR_2022\\thresholds_water.csv': '615ef96f716ef1d43c01ab67f383c881' #> Reading station dictionary from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/stations.txt': '58b9e90f314e89f637c60558c06755f4' #>  #> Reading contaminant and effects data from: #>  'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/water.txt' #> MD5 digest for: 'C:/Users/robfr/Documents/HARSAT/HARSAT/data/example_OSPAR/water.txt': '13d63b6161b671165b215b58f5e22469' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CW or EW  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 2768 records by coordinates #>  - matching 1583 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 4251 of 4351 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2022"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"tidying-the-data","dir":"Articles","previous_headings":"","what":"Tidying the data","title":"Getting started","text":"next step clean data prepare analysis, using tidy_data(). step tidies data structures ’ve got . filtering get data form want say, OSPAR assessment HELCOM assessment. also streamlines data files. may generate warnings. example, ’re cleaning station dictionary, ’ve may find issues duplicate stations. Similarly, comes cleaning contaminant biological effects data, ’ve may find data stations unrecognized station dictionary. Sometimes ’s fine sometimes ’s . Warnings supported output files allow come look see values affected, can go check detail. point, ’ve still done anything datasets.","code":"water_data <- tidy_data(water_data) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Dropping 411 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth >5.5m #>  - filtration missing #>  - matrix = 'SPM' #>  #> Dropping 94 records from data that have no valid station code #>  #> Dropping 13633 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"create-a-time-series","dir":"Articles","previous_headings":"","what":"Create a time series","title":"Getting started","text":"Now can group data time series, using create_timeseries(). means identifying data points belong time series, set structure allows us assessments. various different ways can specify determinants actually want assess. case, determinands parameter specifies CHR, BBKF, PFOS, CB138, HCEPX, HCH. various arguments allow control just data manipulated.","code":"water_timeseries <- create_timeseries(   water_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\"),      HCH = list(det = c(\"HCHA\", \"HCHB\", \"HCHG\"), action = \"sum\")   ) ) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2017 and 2022 #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Implausible uncertainties reported with data: see implausible_uncertainties_reported.csv #>    Data submitted as CHRTR relabelled as CHR  #>    Data submitted as BBF, BKF summed to give BBKF #>      1 of 71 samples lost due to incomplete submissions #>    Data submitted as BBJF, BKF summed to give BBJKF #>      99 of 99 samples lost due to incomplete submissions #>    Data submitted as HCEPC, HCEPT summed to give HCEPX #>    Data submitted as HCHA, HCHB, HCHG summed to give HCH #>      190 of 234 samples lost due to incomplete submissions #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Dropping groups of compounds / stations with no data between 2017 and 2022"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"assessment","dir":"Articles","previous_headings":"","what":"Assessment","title":"Getting started","text":"next next stage assessment, using run_assessment(). ’ve created time series object, pass call. say thresholds ’re going use assessment options can put . just run. can see tells time series ’re actually assessing progresses. gives idea many cups tea can drink ’s finished. various little warnings: ones nothing worry . can check everything converged, using check_assessment().","code":"water_assessment <- run_assessment(   water_timeseries,    AC = \"EQS\",    parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(water_assessment) #> All assessment models have converged"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"reporting","dir":"Articles","previous_headings":"","what":"Reporting","title":"Getting started","text":"can get summary table results. want directory can put . harsat won’t create directory ’s nothing , let’s make new directory, ./output/tutorial, put full path summary.dir, can tell harsat write . can generate summary proper, using write_summary_table(). summary file familiar involved OSPAR HELCOM assessments. summary files information time series, time series represents (first set columns), followed statistical results, p values, summary values number years data set, starts finishes. towards end, ’ve got comparisons various different threshold values.","code":"summary.dir <- file.path(working.directory, \"output\", \"tutorial\")  if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) } write_summary_table(   water_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\", \"PAH_parent\",  \"Organofluorines\",        \"Chlorobiphenyls\", \"Organochlorines\", \"Pesticides\"     ),       labels = c(       \"Metals\", \"Organotins\", \"PAH parent compounds\", \"Organofluorines\",        \"Polychlorinated biphenyls\", \"Organochlorines (other)\", \"Pesticides\"     )   ),   classColour = list(     below = c(\"EQS\" = \"green\"),      above = c(\"EQS\" = \"red\"),      none = \"black\"   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir ) #> Error in write_summary_table(water_assessment, determinandGroups = list(levels = c(\"Metals\", : unused argument (classColour = list(below = c(EQS = \"green\"), above = c(EQS = \"red\"), none = \"black\"))"},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"species-file-format","dir":"Articles","previous_headings":"","what":"Species file format","title":"Reference table file formats","text":"species file species.csv information files directory. can many dry lipid weight columns tissue types. example, blood (BL) data, can columns BL_drywt BL_lipidwt. (Remember difference BL (blood) ER (erythrocytes - red blood cells vertebrates). can also omit columns don’t need convert thresholds basis another.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"determinand-file-format","dir":"Articles","previous_headings":"","what":"Determinand file format","title":"Reference table file formats","text":"determinand file determinand.csv information files directory. yes* mandatory column means e.g. biota_group, biota_assess etc. mandatory biota assessment, can omitted going sediment water assessment.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"threshold-file-format","dir":"Articles","previous_headings":"","what":"Threshold file format","title":"Reference table file formats","text":"threshold files compartment-specific called threshold_biota.csv, threshold_sediment.csv threshold_water.csv. format differs somewhat, principle : provide threshold values determinand. biota, threshold values linked particular species_group, species_subgroup reference_species (see species file format) supporting variables matrix (tissue type). sediment, threshold values can (optionally) linked one regional variables stations file. water, threshold values ar linked filtration. water file simplest, describe first.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"water-threshold-files","dir":"Articles","previous_headings":"Threshold file format","what":"Water threshold files","title":"Reference table file formats","text":"water threshold file thresholds_water.csv information files directory. illustration, suppose just one threshold, EQS. thresholds file following four columns: another threshold, example, BAC, add two columns called BAC_basis BAC. can many thresholds want. , particular determinand, (set ) threshold value(s) applied filtered unfiltered time series, set filtration filtered~unfiltered.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"sediment-threshold-files","dir":"Articles","previous_headings":"Threshold file format","what":"Sediment threshold files","title":"Reference table file formats","text":"sediment threshold file thresholds_sediment.csv information files directory. illustration, suppose two thresholds, BAC EAC. threshold file following five columns: , can many thresholds want. example, OSPAR assessments typically use ERL, EQS FEQG addition BAC EAC. can also extra columns match columns station dictionary. Typically, used apply different threshold values different regions. example, OSPAR threshold file column ospar_subregion, allows one set threshold values applied Iberian Coast Gulf Cadiz another set applied rest OSPAR area.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"biota-threshold-files","dir":"Articles","previous_headings":"Threshold file format","what":"Biota threshold files","title":"Reference table file formats","text":"biota threshold file thresholds_biota.csv information files directory. , suppose two thresholds, BAC EAC. threshold file following eleven columns: columns species_group, species_subgroup, species used provide flexibility matching thresholds species. determinands, threshold applied species species group, case species_group column populated. determinands, threshold values might differ species subgroups (e.g. mussels oysters), ase species_subgroup column populated. Use species column threshold value species-specific. least one species_group, species_subgroup species must always provided.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/report-template.html","id":"through-report_assessment","dir":"Articles","previous_headings":"","what":"Through report_assessment","title":"Report template usage","text":"easiest way generate reports bulk, follows: generate reports, although can pass subsetting expression (just like plot_assessment) select individual assessments want reports . reports written HTML files given output directory.","code":"## Do whatever you need to get the assessment object ... assessment <- run_assessment(timeseries)  report_assessment(   biota_assessment,   subset = NULL,   output_dir = my_output_dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/report-template.html","id":"directly-through-r-markdown","dir":"Articles","previous_headings":"","what":"Directly through R Markdown","title":"Report template usage","text":"case, usage typically look bit like : Note R Markdown, can child document another template. can also control output format, generating PDF, example.","code":"library(rmarkdown)   ## Do whatever you need to get the assessment object ... assessment <- run_assessment(timeseries)  ## Locate the report file package_dir = system.file(package = \"harsat\") template_dir = file.path(package_dir, \"markdown\") report_file <- file.path(template_dir, \"report_assessment.Rmd\")  ## Create a new filename for the output file, here we just ## put it in a temporary file, but you'll usually want something ## more persistent. output_file <- tempfile(\"plot\", fileext = c(\".htm\"))  ## Choose what you want to report. Pick the series you want to ## report like this. params = list(   assessment_object = assessment,   series = 'A902 PFOA Globicephala melas LI JV' )  ## Generate the report -- note the use of `new.env()` to make a  ## nice clean enviroment containing only the parameters you pass, ## as above.  rmarkdown::render(   report_file,   output_file = output_file,   params = params,   envir = new.env() )"},{"path":"http://osparcomm.github.io/HARSAT/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Arctic Monitoring Assessment Programme (AMAP). Copyright holder, funder, author. Helsinki Commission (HELCOM). Copyright holder, funder, author. OSPAR Commission (OSPAR). Copyright holder, funder, maintainer, author. International Council Exploration Sea (ICES). Copyright holder, author. AmbieSense Ltd. Copyright holder, author. HARSAT User Group. Contributor.","code":""},{"path":"http://osparcomm.github.io/HARSAT/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Arctic Monitoring Assessment Programme (AMAP), Helsinki Commission (HELCOM), OSPAR Commission (OSPAR), International Council Exploration Sea (ICES), AmbieSense Ltd (2024). harsat: Harmonized Regional Seas Assessment Tool. R package version 1.0.1.1006, https://osparcomm.github.io/HARSAT/, https://github.com/osparcomm/HARSAT.","code":"@Manual{,   title = {harsat: Harmonized Regional Seas Assessment Tool},   author = {{Arctic Monitoring and Assessment Programme (AMAP)} and {Helsinki Commission (HELCOM)} and {OSPAR Commission (OSPAR)} and {International Council for the Exploration of the Sea (ICES)} and {AmbieSense Ltd}},   year = {2024},   note = {R package version 1.0.1.1006, https://osparcomm.github.io/HARSAT/},   url = {https://github.com/osparcomm/HARSAT}, }"},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"what-is-harsat","dir":"","previous_headings":"","what":"What is HARSAT?","title":"Harmonized Regional Seas Assessment Tool","text":"HARSAT (Harmonised Regional Seas Assessment Tool), tool applied Arctic Monitoring Assessment Programme (AMAP), Helsinki Commission (HELCOM) OSPAR Commission (OSPAR) support assessments data concerning contaminants (hazardous substances) effects marine environment. HARSAT code includes tools pre-processing data, statistical trend analysis comparison threshold values, post-processing archiving reporting. HARSAT developed statistical computing language R. R available operating systems can downloaded R-project website. Disclaimer: HARSAT tool made available Open Source licence. every attempt made ensure HARSAT version(s) developed supported AMAP/HELCOM/OSPAR free errors, use tool third parties, quality products third party use responsibility third party concerned.","code":""},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"system-requirements","dir":"","previous_headings":"","what":"System requirements","title":"Harmonized Regional Seas Assessment Tool","text":"R programming language (version 4.2.1 later). Additional R packages come standard R installation may needed – normally installed automatically, may need permissions tools R Studio (recommended; version 2023.03.1 later). required, HARSAT developers recommend running HARSAT code using RStudio integrated development environment.","code":""},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Harmonized Regional Seas Assessment Tool","text":"full installation details, see Getting Started guide. recommend installing harsat packaged bundle, either using RStudio R command line, ensures dependencies --date, properly downloaded installed.","code":""},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"in-rstudio","dir":"","previous_headings":"Installation","what":"In RStudio","title":"Harmonized Regional Seas Assessment Tool","text":", need downloaded package bundle, typically file called something like harsat_0.1.2.tar.gz. Packages tab, choose Install, make sure selected install Package Archive File, press Browse... button locate bundled package file. finally press Install button.","code":""},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"from-the-r-command-line","dir":"","previous_headings":"Installation","what":"From the R command line","title":"Harmonized Regional Seas Assessment Tool","text":", need downloaded package bundle, typically file called something like harsat_0.1.2.tar.gz. R command line, use command (passing filename wherever downloaded file ):","code":"install.packages(remotes) -- if needed library(remotes) remotes::install_local(\"~/Downloads/harsat_0.1.2.tar\")"},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"directly-from-github","dir":"","previous_headings":"Installation","what":"Directly from Github","title":"Harmonized Regional Seas Assessment Tool","text":"Alternatively, can also install harsat directly GitHub. development version similar, changes often, recommend enjoy exciting time analysis.","code":"library(remotes) remotes::install_github(\"osparcomm/HARSAT@main\") library(remotes) remotes::install_github(\"osparcomm/HARSAT@develop\")"},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"example-usage","dir":"","previous_headings":"","what":"Example usage","title":"Harmonized Regional Seas Assessment Tool","text":"prepared zip files containing files need, OSPAR (subset OSPAR 2022), HELCOM (based HELCOM 2023). zip files contain two directories: data directory information directory. can unzip anywhere like system. Download OSPAR archive Download HELCOM archive Download external data (AMAP) archive Let’s say unzipped file system, C:\\Users\\test\\ospar. , data now C:\\Users\\test\\ospar\\data, reference files C:\\Users\\test\\ospar\\information, although can rename move anywhere like. read data, something like (’re using water OSPAR example – complete example, look full OSPAR example): follow rest process shown Getting Started guide OSPAR example. can, course, start edit files directories choose. Note specific naming conventions files reference file directory especially. Find documentation page file management.","code":"water_data <- read_data(   compartment = \"water\",    purpose = \"OSPAR\",                                  contaminants = \"water.txt\",    stations = \"stations.txt\",    data_dir = file.path(\"C:\", \"users\", \"test\", \"ospar\", \"data\"),         ## i.e., C:\\Users\\test\\ospar\\data   info_dir = file.path(\"C:\", \"users\", \"test\", \"ospar\", \"information\"),  ## i.e., C:\\Users\\test\\ospar\\information   extraction = \"2023/08/23\" )"},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"more-information","dir":"","previous_headings":"","what":"More information","title":"Harmonized Regional Seas Assessment Tool","text":"information, take look Getting Started guide. welcome contributions can make. Check Contributor’s guide .","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":null,"dir":"Reference","previous_headings":"","what":"Add stations to contaminant data from an ICES extraction — add_stations","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"Adds station name station code contaminant data ICES extraction. done either matching station names submitted data station dictionary, matching sample coordinates station dictionary, combination .","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"","code":"add_stations(data, stations, info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"data data frame contaminant data ICES extraction stations data frame ICES station dictionary info HARSAT information list must contain elements purpose, compartment, add_stations. latter list control parameters supplied control_default control_modify control station matching achieved. See details.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"data frame containing contaminant data augmented variables containing station code station name","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"info$add_stations list control parameters modify station matching process: method: string specifying whether stations matched \"name\", \"coordinates\", \"\". info$purpose \"custom\", method restricted either \"name\" (default) \"coordinates\". info$purpose \"OSPAR\", \"HELCOM\" \"AMAP\", method set \"\" default stations matched name coordinates according rules specified OSPAR, HELCOM AMAP data providers. Specifically, stations matched name Denmark, France (biota water - years; sediment 2009 onwards), Ireland, Norway, Portugal, Spain (2005 onwards), Sweden, Netherlands (2007 onwards), United Kingdom. stations matched coordinates. area: vector strings containing one \"OSPAR\", \"HELCOM\" \"AMAP\"; restricts stations corresponding convention area(s); NULL matches stations station dictionary datatype: logical specifying whether stations restricted appropriate datatype. TRUE, contaminant measurement biota (example) matched stations station_datatype containing string \"CF\". Similarly, biological effect measurement biota matched stations station_datatype containing string \"EF\" temporal: logical TRUE indicating stations restricted station_purpm containing string \"T\" governance_type: string: \"none\", \"data\", \"stations\" \"\". \"none\" means data station governance ignored. \"data\" means matching restricted data governance station governance; example governance_id == c(\"OSPAR\", \"AMAP\"), data matched station one is_ospar_monitoring is_amap_monitoring TRUE, stations considered regardless station governance. \"stations\" mean matching restricted station governance data governance; example governance_id == c(\"OSPAR\", \"AMAP\"), stations restricted station_programgovernance contains either \"OSPAR\" \"AMAP\", data considered regardless data governance. uses data station governance. governance_id contains single value, matching strict. However, governance_id contains multiple values, matching complicated. example, governance_id == c(\"OSPAR\", \"AMAP\"), measurements is_ospar_monitoring == TRUE \"is_amap_monitoring == FALSE\" matched stations station_programgovernance contains \"OSPAR\"; measurements is_ospar_monitoring == FALSEandis_amap_monitoring == TRUEare matched stations wherestation_programgovernancecontains\"AMAP\"; measurements is_ospar_monitoring == TRUEandis_amap_monitoring == TRUEare matched stations wherestation_programgovernancecontains either\"OSPAR\"\"AMAP\"`. governance_id: vector strings containing one \"OSPAR\", \"HELCOM\" \"AMAP\". grouping: logical TRUE indicating stations grouped meta-stations specified station_asmtmimeparent station dictionary. Defaults FALSE apart info$purpose == \"OSPAR\". check_coordinates: logical TRUE indicating , stations matched name, sample coordinates must also within station geometry. implemented yet, defaults ot FALSE.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/apply_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply subsetting to a time series — apply_subset","title":"Apply subsetting to a time series — apply_subset","text":"internal function applies subsetting function timeseries. somewhat complex, due way designed called. subset designed passed either value (NULL  logical), expression, variable holding expression. expression, applied context timeseries data frame generate vector booleans subsetting. complexity comes way implements lazy evaluation, evaluate expression normal calling context. function also removes row names, converts series column row names.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/apply_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply subsetting to a time series — apply_subset","text":"","code":"apply_subset(timeSeries, subset, env = parent.frame())"},{"path":"http://osparcomm.github.io/HARSAT/reference/apply_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply subsetting to a time series — apply_subset","text":"timeSeries time series data frame subset (default NULL) either NULL, selects entries, logical, TRUE selects entries FALSE none , expression, evaluated context dataframe generate subsetting vector. env calling environment variable values.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether the assessments have converged — check_assessment","title":"Check whether the assessments have converged — check_assessment","text":"Checks whether assessments HARSAT assessment object converged. Currently detailed checks models normal lognormal errors (chemical timeseries biological effects). checks whether: fixed effect estimates away bounds random effect estimates lower upper bounds; can course equal zero standard errors present model predictions fixed effects estimates standard errors fixed effects estimates realistic (small standard errors indicate problems numerical differencing used compute standard errors)","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether the assessments have converged — check_assessment","text":"","code":"check_assessment(assessment_ob, save_result = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether the assessments have converged — check_assessment","text":"assessment_ob HARSAT assessment object resulting call ctsm_assessment save_result Saves identifiers timeseries converged; defaults FALSE. assessment done stages, output also identifies timeseries yet assessed","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether the assessments have converged — check_assessment","text":"list two character vectors: not_converged identifies timeseries converged not_assessed identifies timeseries assessed","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":null,"dir":"Reference","previous_headings":"","what":"Checks convergence of an assessment model — check_convergence_lmm","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"Utilty function use within assess_lmm. Checks whether model converged. Currently checks assessments normal (lognormal) errors, considers whether: fixed effect estimates away bounds random effect estimates lower upper bounds; can course equal zero standard errors present model predictions fixed effects estimates standard errors fixed effects estimates unrealistically small; tolerance chosen much smaller seen typical OSPAR assessments converged Model fits based distributions assumed converged. checking needed future","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"","code":"check_convergence_lmm(assessment, coeff_se_tol = 0.001)"},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"assessment assessment assess_lmm coeff_se_tol tolerance checking whether standard errors fixed effects estimates unrealistically small; defaults 0.001","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"integer: 0 indicates convergence, 1 indicates issue","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":null,"dir":"Reference","previous_headings":"","what":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"utility function expand simplified OSPAR sediment threshold table (much easier user edit) form required harsat","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"","code":"convert_reftable(input_file, output_file, export = TRUE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"input_file input reference file output_file expanded reference file export boolean flag, FALSE, data returned rather written output_file","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"export FALSE (default), returns expanded data","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_units.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts units, e.g., from mg/kg to ug/kg — convert_units","title":"Converts units, e.g., from mg/kg to ug/kg — convert_units","text":"Can accept non-standard units (e.g. biological effects) provided rows identical (case attempt made convert.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_units.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts units, e.g., from mg/kg to ug/kg — convert_units","text":"","code":"convert_units(conc, from, to)"},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_units.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts units, e.g., from mg/kg to ug/kg — convert_units","text":"conc value convert current units required units","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/create_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a time series — create_timeseries","title":"Create a time series — create_timeseries","text":"Cleans data turns time series structures ready assessment","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/create_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a time series — create_timeseries","text":"","code":"create_timeseries(   ctsm.obj,   determinands = ctsm_get_determinands(ctsm.obj$info),   determinands.control = NULL,   oddity_path = \"oddities\",   return_early = FALSE,   print_code_warnings = FALSE,   get_basis = get_basis_default,   normalise = FALSE,   normalise.control = list() )"},{"path":"http://osparcomm.github.io/HARSAT/reference/create_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a time series — create_timeseries","text":"ctsm.obj CTSM object, returned tidy_data determinands determinands use, default derived calling ctsm_get_determinands, takes values determinand reference table determinands.control determinands control values, needed oddity_path directory write oddities return_early boolean (default FALSE), TRUE, returns early print_code_warnings boolean (default FALSE) get_basis basis function, default get_basis_default normalise boolean function, TRUE, uses default normalization normalise.control list control data normalization function","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/create_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a time series — create_timeseries","text":"completed timeseries object, can used assessments","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/cstm.VDS.environment.html","id":null,"dir":"Reference","previous_headings":"","what":"Detects the environment from the call — cstm.VDS.environment","title":"Detects the environment from the call — cstm.VDS.environment","text":"utility function detects package environment. imported harsat package, returns package environment. Otherwise, returns global environment. can safely export functions result , example, setting cluster child processes parallel computation.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/cstm.VDS.environment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detects the environment from the call — cstm.VDS.environment","text":"","code":"cstm.VDS.environment()"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.cl.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates confidence limits for imposex time seriesl — ctsm.VDS.cl","title":"Calculates confidence limits for imposex time seriesl — ctsm.VDS.cl","text":"Calculates confidence limits imposex time seriesl","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.cl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates confidence limits for imposex time seriesl — ctsm.VDS.cl","text":"","code":"ctsm.VDS.cl(fit, nsim = 1000)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.cl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates confidence limits for imposex time seriesl — ctsm.VDS.cl","text":"fit output call ctsm.VDS.index.opt (sort ) nsim number simulations set confidence limits based; default 1000","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.index.opt.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","title":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","text":"ctsm.VDS.index.opt","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.index.opt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","text":"","code":"ctsm.VDS.index.opt(data, theta, refLevel, calc.vcov = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.index.opt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","text":"data Individual imposex data theta Maximum imposex stage refLevel (optional) reference level parameter estimation; defaults index intermediate levels imposex calc.vcov Logical specifying whether calculate covariance matrix parameter estimates; defaults FALSE","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.loglik.calc.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","title":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","text":"ctsm.VDS.loglik.calc","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.loglik.calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","text":"","code":"ctsm.VDS.loglik.calc(   theta,   data,   index.theta,   minus.twice = FALSE,   cumulate = FALSE )"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.loglik.calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","text":"theta maximum imposex stage data Individual imposex data. index.theta Allows stage names run 0 theta (optional) minus.twice logical specifying whehter calculated liklihood (FALSE) deviance (TRUE); default FALSE cumulate logical specifying whether use cumulative probabilities; default FALSE","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.p.calc.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","title":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","text":"ctsm.VDS.p.calc","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.p.calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","text":"","code":"ctsm.VDS.p.calc(theta, cumulate = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.p.calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","text":"theta vector values cumulate boolean, whether use cumulative probabilities","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.varlist.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.varlist — ctsm.VDS.varlist","title":"ctsm.VDS.varlist — ctsm.VDS.varlist","text":"list names functions values necessary export cluster prcesses parallel computation","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.varlist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.varlist — ctsm.VDS.varlist","text":"","code":"ctsm.VDS.varlist"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.varlist.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ctsm.VDS.varlist — ctsm.VDS.varlist","text":"object class character length 4.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.projection.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates a polar projection — ctsm.projection","title":"Calculates a polar projection — ctsm.projection","text":"Calculates easting northing Lambert Azimuthal Equal Area North Polar aspect projection","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.projection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates a polar projection — ctsm.projection","text":"","code":"ctsm.projection(latitude, longitude)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.projection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates a polar projection — ctsm.projection","text":"latitude latitude longitude longitude","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.projection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates a polar projection — ctsm.projection","text":"list projected longitude (easting) latitude (northing) values","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_TBT_convert.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert tin concentrations — ctsm_TBT_convert","title":"Convert tin concentrations — ctsm_TBT_convert","text":"Convert tin concentrations cation concentrations.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_TBT_convert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert tin concentrations — ctsm_TBT_convert","text":"","code":"ctsm_TBT_convert(   data,   subset,   action,   from = c(\"tin\", \"cation\"),   convert_var = c(\"value\", \"limit_detection\", \"limit_quantification\", \"uncertainty\") )"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_TBT_convert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert tin concentrations — ctsm_TBT_convert","text":"data data subset optional, subset expression action one \"relabel\", \"convert\", \"change_unit\" – note change_unit moves units tin units conventional units either \"tin\" \"cation\" convert_var one \"value\", \"limit_detection\", \"limit_quantification\", \"uncertainty\"","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_convert_basis.html","id":null,"dir":"Reference","previous_headings":"","what":"Basis and matrix information and basis conversion — ctsm_convert_basis","title":"Basis and matrix information and basis conversion — ctsm_convert_basis","text":"Converts concentrations wet, dry lipid bases","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_convert_basis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Basis and matrix information and basis conversion — ctsm_convert_basis","text":"","code":"ctsm_convert_basis(   conc,   from,   to,   drywt = NA_real_,   lipidwt = NA_real_,   drywt_censoring = ifelse(is.na(drywt), NA_character_, \"\"),   lipidwt_censoring = ifelse(is.na(lipidwt), NA_character_, \"\"),   exclude = FALSE,   print_warning = TRUE )"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_convert_basis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Basis and matrix information and basis conversion — ctsm_convert_basis","text":"conc concentration current basis target basis drywt assumed percentage taking values [0, 100] lipidwt assumed percentage taking values [0, 100]; assumed wet weight basis drywt_censoring required drywt present, character: must ”, '<', 'D', 'Q' NA lipidwt_censoring required lipidwt present, character: must ”, '<', 'D', 'Q' NA exclude logical identifying records need converted, useful data contain biological effects measurements print_warning boolean, gives number records lost conversion","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the determinands to be assessed — ctsm_get_determinands","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"Gets determinands assessed determinand reference table.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"","code":"ctsm_get_determinands(info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"info list least following two components: compartment One \"biota\", \"sediment\" \"water\" determinand data frame holding determinand reference table","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"character string containing determinands assessed. function fail error message determinands.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"determinands taken column biota_assess, sediment_assess water_assess determinand reference table (compartment given info$compartment). TRUE values determinands assessed.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Gets data from the standard reference tables — ctsm_get_info","title":"Gets data from the standard reference tables — ctsm_get_info","text":"Gets data standard reference tables","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gets data from the standard reference tables — ctsm_get_info","text":"","code":"ctsm_get_info(   ref_table,   input,   output,   compartment = NULL,   na_action = c(\"fail\", \"input_ok\", \"output_ok\", \"ok\"),   info_type = NULL,   sep = \".\" )"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gets data from the standard reference tables — ctsm_get_info","text":"ref_table reference table input input output output compartment compartment na_action character, handle missing values, one : fail (missing values allowed), input_ok (allows missing values input, non-missing values must recognised, must output), output_ok`` requires input values present, allows missing values  output (e.g. dryweight species), ok` (allows missing values everywhere) info_type information type, default picked function call sep character, separator, defaulting '.'","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_uncrt_plot_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a relative uncertainty plot — ctsm_uncrt_plot_data","title":"Generates a relative uncertainty plot — ctsm_uncrt_plot_data","text":"Generates relative uncertainty plot","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_uncrt_plot_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a relative uncertainty plot — ctsm_uncrt_plot_data","text":"","code":"ctsm_uncrt_plot_data(uncrt_obj, det_id)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_uncrt_plot_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a relative uncertainty plot — ctsm_uncrt_plot_data","text":"uncrt_obj uncertainty object det_id list determinand ids","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC.html","id":null,"dir":"Reference","previous_headings":"","what":"Access function map — get_AC","title":"Access function map — get_AC","text":"Access function map","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access function map — get_AC","text":"","code":"get_AC"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Access function map — get_AC","text":"object class list length 3.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC_biota_OSPAR.html","id":null,"dir":"Reference","previous_headings":"","what":"Gets OSPAR threshold values for biota — get_AC_biota_OSPAR","title":"Gets OSPAR threshold values for biota — get_AC_biota_OSPAR","text":"Extends default extractor function get_AC$biota cases can handled straightforward manner. mostly relates thresholds applied typical species / lipid contend 'high' (currently defined >= 3%)","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC_biota_OSPAR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gets OSPAR threshold values for biota — get_AC_biota_OSPAR","text":"","code":"get_AC_biota_OSPAR(data, AC, rt, export_all = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC_biota_OSPAR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gets OSPAR threshold values for biota — get_AC_biota_OSPAR","text":"data data AC list assessment criteria rt reference tables export_all logical, default FALSE, TRUE returns data, otherwise, just assessment criteria","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC_biota_OSPAR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gets OSPAR threshold values for biota — get_AC_biota_OSPAR","text":"default, assessment criteria, export_all set TRUE returns data instead","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_biota_OSPAR.html","id":null,"dir":"Reference","previous_headings":"","what":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","title":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","text":"Gets OSPAR biota target basis","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_biota_OSPAR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","text":"","code":"get_basis_biota_OSPAR(data, info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_biota_OSPAR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","text":"data data info information object","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_default.html","id":null,"dir":"Reference","previous_headings":"","what":"The default function for generating a basis — get_basis_default","title":"The default function for generating a basis — get_basis_default","text":"default simplistic, works cases sediment water most_common used AMAP mercury assessment takes common basis reported station, species (biota), matrix determinand_group combination","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The default function for generating a basis — get_basis_default","text":"","code":"get_basis_default(data, info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The default function for generating a basis — get_basis_default","text":"data data info information object","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_station_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Get station code from station name — get_station_code","title":"Get station code from station name — get_station_code","text":"Gets station code corresponding station name country station dictionary.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_station_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get station code from station name — get_station_code","text":"","code":"get_station_code(station_name, country, stations)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_station_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get station code from station name — get_station_code","text":"station_name station name country country stations station dictionary","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts timeSeries — get_timeseries","title":"Extracts timeSeries — get_timeseries","text":"Gets timeSeries component harsat object, optionally added extra information station","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts timeSeries — get_timeseries","text":"","code":"get_timeseries(harsat_obj, add = TRUE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts timeSeries — get_timeseries","text":"harsat_obj harsat object following call create_timeseries`. add logical (default TRUE), TRUE, adds extra information station; FALSE, simply returns existing timeseries,","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts timeSeries — get_timeseries","text":"data.frame containing timeSeries component (optionally) extra information station.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/harsat-package.html","id":null,"dir":"Reference","previous_headings":"","what":"harsat: Harmonized Regional Seas Assessment Tool — harsat-package","title":"harsat: Harmonized Regional Seas Assessment Tool — harsat-package","text":"Tools assessment data concerning contaminants (hazardous substances) effects marine environment. code includes tools pre-processing data, statistical trend analysis comparison threshold values, post-processing archiving reporting.","code":""},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/reference/harsat-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"harsat: Harmonized Regional Seas Assessment Tool — harsat-package","text":"Maintainer: OSPAR Commission (OSPAR) data@ospar.org [copyright holder, funder] Authors: Arctic Monitoring Assessment Programme (AMAP) amap@amap.[copyright holder, funder] Helsinki Commission (HELCOM) secretariat@helcom.fi [copyright holder, funder] International Council Exploration Sea (ICES) info@ices.dk [copyright holder] AmbieSense Ltd info@ambiesense.com [copyright holder] contributors: HARSAT User Group harsat@ospar.org [contributor]","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/info_TEF.html","id":null,"dir":"Reference","previous_headings":"","what":"TEFs for selected groups of compounds — info_TEF","title":"TEFs for selected groups of compounds — info_TEF","text":"list named vectors provide Toxic Equivalency Factors (TEFs) calculating Toxic EQuivalent (TEQ) sums.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/info_TEF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TEFs for selected groups of compounds — info_TEF","text":"","code":"info_TEF$DFP_2005 info_TEF$DFP_2022"},{"path":"http://osparcomm.github.io/HARSAT/reference/info_TEF.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"TEFs for selected groups of compounds — info_TEF","text":"list named vectors: DFP_2005 gives 2005 World Health Organisation TEFs dioxins, furans dioxin-like (planar) polychlorinated biphenyls (Van den Berg et al. 2006) DFP_2022 gives 2022 World Health Organisation TEFs dioxins, furans dioxin-like (planar) polychlorinated biphenyls (DeVito et al. 2024) DFP_HOLAS3 superseded version DFP_2005 used HELCOM HOLAS3 assessment. excludes CB114, CB123 CB189 uses code CDFO instead OCDF. included reproducibility, might removed future. DFP_CEMP superseded version DFP_2005 used OSPAR CEMP assessments including 2024. excludes CB114, CB123 CB189, uses code CDFO instead OCDF, TEF CDFO 0.00003 rather 0.0003. included reproducibility, might removed future.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/info_TEF.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"TEFs for selected groups of compounds — info_TEF","text":"DeVito M, Bokkers B, van Duursen MBM, van Ede K, Feeley M, Gáspár EAF, Haws L, Kennedy S, Peterson RE, Hoogenboom R, Nohara K, Petersen K, Rider C, Rose M, Safe S, Schrenk D, Wheeler MW, Wikoff DS, Zhao B, van den Berg M, 2024. 2022 world health organization reevaluation human mammalian toxic equivalency factors polychlorinated dioxins, dibenzofurans biphenyls. Regulatory Toxicology Pharmacology 146; https://doi.org/10.1016/j.yrtph.2023.105525 Van den Berg M, Birnbaum LS, Denison M, De Vito M, Farland W, Feeley M, Fiedler H, Hakansson H, Hanberg , Haws L, Rose M, Safe S, Schrenk D, Tohyama C, Tritscher , Tuomisto J, Tysklind M, Walker N, Peterson RE, 2006. 2005 World Health Organization reevaluation human mammalian toxic equivalency factors dioxins dioxin-like compounds. Toxicological Sciences 93 223–241; https://doi.org/10.1093/toxsci/kfl055","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the path for an information file — locate_information_file","title":"Find the path for an information file — locate_information_file","text":"Locates requested information file, searching information file path. requested file found required false, stops error.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the path for an information file — locate_information_file","text":"","code":"locate_information_file(name, path)"},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the path for an information file — locate_information_file","text":"name string: name file, e.g., thresholds_biota.csv path vector strings, directories search. information directory package automatically searched found file anywhere else","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find the path for an information file — locate_information_file","text":"string, absolute path file, NULL file found anywhere.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_biota_HELCOM.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalises biota concentrations, HELCOM vwersion — normalise_biota_HELCOM","title":"Normalises biota concentrations, HELCOM vwersion — normalise_biota_HELCOM","text":"Normalises biota concentrations, HELCOM vwersion","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_biota_HELCOM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalises biota concentrations, HELCOM vwersion — normalise_biota_HELCOM","text":"","code":"normalise_biota_HELCOM(data, station_dictionary, info, control)"},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_biota_HELCOM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalises biota concentrations, HELCOM vwersion — normalise_biota_HELCOM","text":"data data object station_dictionary station dictionary info information object control control values","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_sediment_HELCOM.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalises sediment concentrations, HELCOM version — normalise_sediment_HELCOM","title":"Normalises sediment concentrations, HELCOM version — normalise_sediment_HELCOM","text":"Normalises sediment concentrations, HELCOM version","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_sediment_HELCOM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalises sediment concentrations, HELCOM version — normalise_sediment_HELCOM","text":"","code":"normalise_sediment_HELCOM(data, station_dictionary, info, control)"},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_sediment_HELCOM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalises sediment concentrations, HELCOM version — normalise_sediment_HELCOM","text":"data data object station_dictionary station dictionary info information object control control values","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_sediment_OSPAR.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalises sediment concentrations, OSPAR vwersion — normalise_sediment_OSPAR","title":"Normalises sediment concentrations, OSPAR vwersion — normalise_sediment_OSPAR","text":"Normalises sediment concentrations, OSPAR vwersion","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_sediment_OSPAR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalises sediment concentrations, OSPAR vwersion — normalise_sediment_OSPAR","text":"","code":"normalise_sediment_OSPAR(data, station_dictionary, info, control)"},{"path":"http://osparcomm.github.io/HARSAT/reference/normalise_sediment_OSPAR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalises sediment concentrations, OSPAR vwersion — normalise_sediment_OSPAR","text":"data data object station_dictionary station dictionary info information object control control values","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Graphical summaries of an assessment — plot_assessment","title":"Graphical summaries of an assessment — plot_assessment","text":"Generates series assessment plots time series. plots exported either png pdf files.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graphical summaries of an assessment — plot_assessment","text":"","code":"plot_assessment(   assessment_obj,   subset = NULL,   output_dir = \".\",   file_type = c(\"data\", \"index\", \"auxiliary\"),   file_format = c(\"png\", \"pdf\"),   auxiliary = \"default\" )"},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graphical summaries of an assessment — plot_assessment","text":"assessment_obj assessment object resulting call run_assessment subset optional vector specifying timeseries plotted. expression evaluated timeSeries component assessment_obj; use series identify individual timeseries. output_dir output directory assessment plots (possibly supplied using file.path). default working directory. output directory must already exist. file_type character vector specifying types assessment plot. default c(\"data\", \"index\", \"auxiliary\") produces three plots time series. See details file_format character string specifying Whether files png (default) pdf. auxiliary character string specifying auxiliary variables plotted file_type = \"auxiliary\". See details","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Graphical summaries of an assessment — plot_assessment","text":"series png pdf files graphical summaries assessment.","code":""},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"types-of-assessment-plots","dir":"Reference","previous_headings":"","what":"Types of assessment plots","title":"Graphical summaries of an assessment — plot_assessment","text":"file_type = \"data\" shows raw data fitted trend pointwise two-sided 90% confidence bands file_type = \"index\" shows annual indices summarise data year fitted trend pointwise two-sided 90% confidence bands file_type = \"auxiliary\" shows raw data key auxiliary variables see )","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"auxiliary-variables","dir":"Reference","previous_headings":"","what":"Auxiliary variables","title":"Graphical summaries of an assessment — plot_assessment","text":"default (auxiliary = \"default\") plot following variables: biota: determinand concentration, LNMEA (mean length), DRYWT% (dry weight content), LIPIDWT% (lipi weight content) sediment: non-normalised determinand concentration, normalised determinand concentration, AL (aluminium concentration), CORG (organic carbon content) water: plots generated present biota, determinand concentration always plotted, possible change three auxiliary variables. example, plot WTMEA (mean weight) instead LIPIDWT% set auxiliary =  c(\"LNMEA\", \"WTMEA\", \"DRYWT%). work, WTMEA must previously specified auxiliary variable determinand question using biota_auxliary column determinand reference table. present, must always three auxiliary variables biota. sediment, non-normalised determinand concentration normalised determinand concentration always plotted, possible change two auxiliary variables. example, metals sediment, might set auxiliary = c(\"AL\", \"LI\") plot aluminium lithium concentrations instead aluminium organic carbon concentrations. , work, LI must previously specified auxiliary variable determinand question using sediment_auxliary column determinand reference table. present, must always two auxiliary variables sediment. present, plots limited range auxiliary variables supported. flexibility plots, changing number auxiliary variables, desirable emerge due course.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads contaminant data — read_contaminants","title":"Reads contaminant data — read_contaminants","text":"quick way reading contaminant data (also avoids station matching info$data_format == \"ICES).","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads contaminant data — read_contaminants","text":"","code":"read_contaminants(file, data_dir = \".\", info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads contaminant data — read_contaminants","text":"file file reference contaminant data. data_dir path directory holding contaminant data. Defaults working directory. info list containing least following two elements: compartment: \"biota\", \"sediment\" \"water\" data_format: \"ICES\" \"external\"","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads contaminant data — read_contaminants","text":"data frame containing contaminant data.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Read HARSAT data — read_data","title":"Read HARSAT data — read_data","text":"Reads contaminant effects data, station dictionary various reference tables. data ICES webservice, matches data stations station dictionary.  also allows user set control parameters dictate assessment process.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read HARSAT data — read_data","text":"","code":"read_data(   compartment = c(\"biota\", \"sediment\", \"water\"),   purpose = c(\"OSPAR\", \"HELCOM\", \"AMAP\", \"custom\"),   contaminants,   stations,   data_dir = \".\",   data_format = c(\"ICES\", \"external\"),   info_files = list(),   info_dir = \".\",   extraction = NULL,   max_year = NULL,   oddity_dir = \"oddities\",   control = list() )"},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read HARSAT data — read_data","text":"compartment string: \"biota\", \"sediment\" \"water\" purpose string specifying whether use default set \"OSPAR\", \"HELCOM\", \"AMAP\" use customised setup \"custom\" contaminants file reference contaminant data stations file reference station data data_dir directory data files can found (sometimes supplied using 'file.path'). Defaults \".\"; .e. working directory. data_format string specifying whether data extracted ICES webservice (\"ICES\" - default) simplified format designed data sources (\"external\"). info_files list files specifying reference tables override defaults. See examples. info_dir directory reference tables can found (sometimes supplied using 'file.path'). Defaults \".\"; .e. working directory extraction date saying extraction made. Optional. provided according ISO 8601; example, 29 February 2024 supplied \"2024-02-29\". contaminant data extracted ICES webservice download file name changed, extraction data taken contaminant file name. max_year integer giving last monitoring year included assessment. Data monitoring years max_year deleted. specified max_year taken last monitoring year contaminant data file. oddity_dir directory 'oddities' written (sometimes supplied using 'file.path'). directory (subdirectories) created already exist. control list control parameters override default values used run assessment. include reporting window; way data matched stations following ICES extraction; information reporting regions, . See Details.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read HARSAT data — read_data","text":"list following components: call function call. info list containing reference tables control parameters. data data frame containing contaminant (effects) data. external data, identical input data file apart extra empty columns added. ICES data, existing columns renamed (otherwise untouched) additional columns constructed. key ones : station_code code station station dictionary best matches data station_name name station species (biota) species based worms_accepted_name available speci_name otherwise filtration (water) whether sample filtered unfiltered based method_pretreatment retain logical indicating whether record retained previous ICES extraction protocol. example, retain FALSE vflag entry \"S\" suspect. Records retain == FALSE deleted later tidy_data stations","code":""},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"control-parameters","dir":"Reference","previous_headings":"","what":"Control parameters","title":"Read HARSAT data — read_data","text":"Many aspects assessment process can controlled parameters stored info$control. list populated default values can overwritten, required, using control argument.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"external-data","dir":"Reference","previous_headings":"","what":"External data","title":"Read HARSAT data — read_data","text":"data_format = \"external\", simplified data station file can supplied. See vignette(\"external-file-format\") details.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads station dictionary — read_stations","title":"Reads station dictionary — read_stations","text":"Reads station dictionary","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads station dictionary — read_stations","text":"","code":"read_stations(file, data_dir = \".\", info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads station dictionary — read_stations","text":"file file reference station dictionary. data_dir path directory holding station dictionary. Defaults working directory. info list containing least following two elements: compartment: \"biota\", \"sediment\" \"water\" data_format: \"ICES\" \"external\"","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads station dictionary — read_stations","text":"data frame containing station dictionary.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/report_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Reports the assessment of individual time series — report_assessment","title":"Reports the assessment of individual time series — report_assessment","text":"Generates series html reports , time series, meta data, plots data fitted assessment model, statistical summaries, simple interpretation fitted model.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/report_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reports the assessment of individual time series — report_assessment","text":"","code":"report_assessment(   assessment_obj,   subset = NULL,   output_dir = \".\",   output_file = NULL,   max_report = 100L )"},{"path":"http://osparcomm.github.io/HARSAT/reference/report_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reports the assessment of individual time series — report_assessment","text":"assessment_obj assessment object resulting call run_assessment subset optional vector specifying timeseries reported. expression evaluated timeSeries component assessment_obj; use 'series' identify individual timeseries. output_dir output directory assessment plots (possibly supplied using 'file.path'). default working directory. output directory must already exist. output_file alterntive file name override default. currently implemented single report. supplied, .html extension added. max_report maximum number reports generated. Defaults 100. report 1MB size takes seconds run, prevents ridiculous number reports created.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/report_assessment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reports the assessment of individual time series — report_assessment","text":"series html files , time series, meta data, plots data fitted assessment model, statistical summaries, simple interpretation fitted model.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/report_file_digest.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates and logs a file digest — report_file_digest","title":"Generates and logs a file digest — report_file_digest","text":"Writes line console containing file name MD5 digest. can used track changes input files, reproducibility reasons. MD5 good enough cryptograohic levels security, cheaper compute.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/report_file_digest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates and logs a file digest — report_file_digest","text":"","code":"report_file_digest(infile)"},{"path":"http://osparcomm.github.io/HARSAT/reference/report_file_digest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates and logs a file digest — report_file_digest","text":"infile input file name","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Assess timeseries for trends and status — run_assessment","title":"Assess timeseries for trends and status — run_assessment","text":"Fits model timeseries, test temporal trend compare thresholds. Need add lot details.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assess timeseries for trends and status — run_assessment","text":"","code":"run_assessment(   ctsm_ob,   subset = NULL,   AC = NULL,   get_AC_fn = NULL,   recent_trend = 20L,   parallel = FALSE,   extra_data = NULL,   control = list(),   ... )"},{"path":"http://osparcomm.github.io/HARSAT/reference/run_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assess timeseries for trends and status — run_assessment","text":"ctsm_ob HARSAT object resulting call create_timeSeries subset optional vector specifying timeseries assessed. Might used assessment done chunks size, refitting timeseries model converged. expression evaluated timeSeries component ctsm_ob; use 'series' identify individual timeseries. AC character vector identifying thresholds used status assessments. threshold reference table. Defaults NULL; .e. thresholds used. get_AC_fn optional function overrides get_AC_default. See details (need written). recent_trend integer giving number years used assessment recent trends. example, value 20 (default) consider trends last twenty year. parallel logical determines whether use parallel computation; default = FALSE. extra_data named list used pass additional data specific assessment routines. present used imposex assessments, passes two data frames called VDS_estimates VDS_confidence_limits. Defaults NULL, argument generalised near future, expect change. control list control parameters allow user modify way assessment run. present, include parameters involved post-hoc power calculations, intended move structures recent_trend . See details (need written). ... Extra arguments passed assessment_engine. See details (need written).","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_default.html","id":null,"dir":"Reference","previous_headings":"","what":"Default control parameters for run_assessment — run_control_default","title":"Default control parameters for run_assessment — run_control_default","text":"Default parameters control way assessment run. Presently includes parameters post-hoc power, intended move recent_trend , along arguments control calculation numerical derivatives.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default control parameters for run_assessment — run_control_default","text":"","code":"run_control_default()"},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_default.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Default control parameters for run_assessment — run_control_default","text":"list following components: power list following components (expressed percentages): target_power default = 90% target_trend default = 5% size default = 5% power calculations currently applied log-normally distributed data, trend expressed percentage.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_modify.html","id":null,"dir":"Reference","previous_headings":"","what":"Modifies control parameters for run_assessment — run_control_modify","title":"Modifies control parameters for run_assessment — run_control_modify","text":"Undates default control parameters user specification basis error checking.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_modify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modifies control parameters for run_assessment — run_control_modify","text":"","code":"run_control_modify(control_default, control = list())"},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_modify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modifies control parameters for run_assessment — run_control_modify","text":"control List replacement control parameters; defaults empty list. run_control_default List default control parameters produced call run_control_default","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_control_modify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modifies control parameters for run_assessment — run_control_modify","text":"List updated control parameters","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/safe_read_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads an input file, given a particular encoding, and possibly additional hints — safe_read_file","title":"Reads an input file, given a particular encoding, and possibly additional hints — safe_read_file","text":"Reads input file, given particular encoding, possibly additional hints","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/safe_read_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads an input file, given a particular encoding, and possibly additional hints — safe_read_file","text":"","code":"safe_read_file(   file,   header = TRUE,   sep = \",\",   quote = \"\\\"\",   dec = \".\",   fill = TRUE,   comment.char = \"\",   strip.white = TRUE,   ... )"},{"path":"http://osparcomm.github.io/HARSAT/reference/safe_read_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads an input file, given a particular encoding, and possibly additional hints — safe_read_file","text":"file file name header logical (default TRUE), whether header line expected sep character (defailt ,), field separator character quote character (default \"), field quote character dec character (default .), decimal character fill logical (default TRUE), rows unequal lengtg, additional blank fields added comment.char character (default empty), empty, comment character strip.white local (default TRUE), strips leading trailing white space fields ... additional parameters utils::read.table","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/safe_read_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads an input file, given a particular encoding, and possibly additional hints — safe_read_file","text":"data frame","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/subset_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Subsets an assessment object — subset_assessment","title":"Subsets an assessment object — subset_assessment","text":"Selects specific time series simplifies data, stations assessment components match","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/subset_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subsets an assessment object — subset_assessment","text":"","code":"subset_assessment(assessment_obj, subset)"},{"path":"http://osparcomm.github.io/HARSAT/reference/subset_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subsets an assessment object — subset_assessment","text":"assessment_obj assessment object resulting call run_assessment. subset vector specifying timeseries retained. expression evaluated timeSeries component assessment_obj; use 'series' identify individual timeseries.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/subset_assessment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subsets an assessment object — subset_assessment","text":"new assessment object, applying subset","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/tidy_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy the data — tidy_data","title":"Tidy the data — tidy_data","text":"Reduces size extraction removing redundant variables. ad-hoc changes usually made read_data tidy_data. output correct format create_timeseries.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/tidy_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidy the data — tidy_data","text":"","code":"tidy_data(ctsm_obj)"},{"path":"http://osparcomm.github.io/HARSAT/reference/tidy_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidy the data — tidy_data","text":"ctsm_obj time series data, typically returned read_data.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/update_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Update timeseries assessments — update_assessment","title":"Update timeseries assessments — update_assessment","text":"Refits models particular timeseries, fits new models assessment done chunks.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/update_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update timeseries assessments — update_assessment","text":"","code":"update_assessment(ctsm_ob, subset = NULL, parallel = FALSE, ...)"},{"path":"http://osparcomm.github.io/HARSAT/reference/update_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update timeseries assessments — update_assessment","text":"ctsm_ob HARSAT object resulting call run_assessment subset vector specifying timeseries assessements updated fit first time. expression evaluated timeSeries component ctsm_ob; use 'series' identify individual timeseries. parallel logical determines whether use parallel computation; default = FALSE. ... Extra arguments passed assessment_engine.  See details (need written).","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Write assessment summary to a csv file — write_summary_table","title":"Write assessment summary to a csv file — write_summary_table","text":"Creates data frame summarising assessment time series writes csv file. summary includes: meta-data monitoring location number years data time series fitted values last monitoring year associated upper one-sided 95% confidence limits trend assessments (p-values trend estimates) status assessments (thresholds) (optionally) symbology summarising trend (shape) status (colour) time series. experimental.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write assessment summary to a csv file — write_summary_table","text":"","code":"write_summary_table(   assessment_obj,   output_file = NULL,   output_dir = \".\",   export = TRUE,   collapse_AC = NULL,   extra_output = NULL,   symbology = NULL,   determinandGroups = NULL,   append = FALSE )"},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write assessment summary to a csv file — write_summary_table","text":"assessment_obj assessment object resulting call run_assessment. output_file name output csv file. using NULL, file called biota_summary.csv, sediment_summary.csv water_summary.csv appropriate. default file written working directory. file name provided, path output file can also provided (e.g. using file.path). `output_dir“ option can also used specify output file directory. output_dir output directory output_file. default working directory. file path provided output_file, appended output_dir. resulting output directory must already exist. export Logical. TRUE (default) writes summary table csv file. FALSE returns summary table R object (write csv file). collapse_AC names list valid assessment criteria allows assessment criteria 'type' reported together. See details. extra_output character vector specifying extra summary metrics included output. Currently recognises \"power\" give seven power metrics computed lognormally distributed data. Defaults NULL; .e. extra output. symbology Experimental. Specifies output symbology. Currently assumes thresholds presented increasing magnitude environmental risk. determinandGroups optional, list specifying labels levels label determinands append Logical. FALSE (default) overwrites existing summary file. TRUE appends data , creating yet exist.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write assessment summary to a csv file — write_summary_table","text":"summary object, export FALSE","code":""}]
